## 1.データ生成

参考　https://www.mhlw.go.jp/content/nenreikaikyubetsu-vaccination_data.pdf



```python
import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.utils import resample
from sklearn.pipeline import Pipeline
from sklearn.tree import DecisionTreeClassifier, export_graphviz
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, precision_recall_fscore_support, ConfusionMatrixDisplay
import graphviz
from dtreeviz.trees import dtreeviz
```


```python
# シードを固定して再現性を確保
np.random.seed(42)

# サンプルデータの生成
n_samples = 10000

# 特徴量とターゲットの生成
V1 = np.random.randint(20, 80, n_samples)
V2 = np.random.binomial(1, np.clip((V1 - 20) * 0.03, 0, 0.95))
V3 = np.random.choice([0, 1], n_samples)
V4 = np.random.choice([0, 1], n_samples, p=[0.7, 0.3])
V5 = np.random.choice([0, 1], n_samples, p=[0.8, 0.2])
V6 = np.random.choice([0, 1], n_samples, p=[0.7, 0.3])
V7 = np.random.choice([0, 1], n_samples, p=[0.9, 0.1])

# リスクスコアを計算する
# V1の値を基準とした基本スコアを計算
risk_score = np.clip((V1 - 20) * 0.005, 0, 0.2)

# 他の特徴量による影響を加味してスコアを調整
risk_score *= (1 + 2 * V4 + 0.5 * V5 + 1.5 * V6 + 2 * V7 + 0.1 * V3)

# V2の値によるリスク軽減効果を反映
risk_score *= (1 - 0.5 * V2)

# スコアを0から1の範囲に制限
risk_score = np.clip(risk_score, 0, 1)

# リスクスコアに基づいてターゲット値を決定
Target = np.random.binomial(1, risk_score)

# データフレームにまとめる
data = pd.DataFrame({
    'V1': V1,
    'V2': V2,
    'V3': V3,
    'V4': V4,
    'V5': V5,
    'V6': V6,
    'V7': V7,
    'Target': Target
})

# カテゴリ変数の追加
# V1を範囲に基づいてグループ化
data['V1_Category'] = pd.cut(V1, bins=[19, 30, 40, 50, 60, 70, 80],
    labels=['20-30', '30-40', '40-50', '50-60', '60-70', '70-80'])

# データフレームをCSVファイルとして保存
output_path = './decision_tree_input/generated_data_anonymized.csv'
os.makedirs(os.path.dirname(output_path), exist_ok=True)
data.to_csv(output_path, index=False)

# データの確認
data.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>V1</th>
      <th>V2</th>
      <th>V3</th>
      <th>V4</th>
      <th>V5</th>
      <th>V6</th>
      <th>V7</th>
      <th>Target</th>
      <th>V1_Category</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>58</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>50-60</td>
    </tr>
    <tr>
      <th>1</th>
      <td>71</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>70-80</td>
    </tr>
    <tr>
      <th>2</th>
      <td>48</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>40-50</td>
    </tr>
    <tr>
      <th>3</th>
      <td>34</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>30-40</td>
    </tr>
    <tr>
      <th>4</th>
      <td>62</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>60-70</td>
    </tr>
  </tbody>
</table>
</div>



## 2. データの可視化


```python
# 出力ディレクトリ
output_dir = './decision_tree_output/'
os.makedirs(output_dir, exist_ok=True)

# 各要因とターゲットの関係を可視化
factors = ['V2', 'V3', 'V4', 'V5', 'V6', 'V7']

for factor in factors:
    plt.figure(figsize=(12, 6))
    factor_target_rate = data.groupby(['V1_Category', factor])['Target'].mean().reset_index()
    sns.barplot(x='V1_Category', y='Target', hue=factor, data=factor_target_rate, palette='viridis')
    plt.title(f'Target Rate by {factor} and V1 Category')
    plt.ylabel('Target Rate')
    plt.xlabel('V1 Category')
    filename = f"{output_dir}target_rate_by_{factor.lower()}_and_v1_category.svg"
    plt.savefig(filename, format='svg')
    plt.show()

# V2とターゲットの関係をV1カテゴリを無視して可視化
target_rate_by_v2 = data.groupby('V2')['Target'].mean().reset_index()
plt.figure(figsize=(8, 6))
sns.barplot(x='V2', y='Target', hue='V2', data=target_rate_by_v2, palette='viridis', dodge=False)
plt.title('Target Rate by V2 (Without V1 Stratification)')
plt.ylabel('Target Rate')
filename = f"{output_dir}target_rate_by_v2.svg"
plt.savefig(filename, format='svg')
plt.show()

```


    
![png](output_5_0.png)
    



    
![png](output_5_1.png)
    



    
![png](output_5_2.png)
    



    
![png](output_5_3.png)
    



    
![png](output_5_4.png)
    



    
![png](output_5_5.png)
    



    
![png](output_5_6.png)
    


## 3. 決定木モデルの作成と評価

### オーバーサンプリング 


```python
# V1_Category列を削除
data = data.drop(columns=['V1_Category'])

# 特徴量とターゲットの分離
X = data.drop('Target', axis=1)
y = data['Target']

# データの分割
X_train, X_test, y_train, y_test = train_test_split(X, y, 
                                    test_size=0.2, random_state=42)

# トレーニングデータで少数クラスのオーバーサンプリング
class_0 = X_train[y_train == 0]
class_1 = X_train[y_train == 1]
y_class_0 = y_train[y_train == 0]
y_class_1 = y_train[y_train == 1]

X_class_1_upsampled, y_class_1_upsampled = resample(
                        class_1,
                        y_class_1,
                        replace=True,  # サンプルを複製
                        n_samples=len(class_0),  # クラス0と同じ数にする
                        random_state=42)  # 乱数シード

X_train_balanced = pd.concat([class_0, X_class_1_upsampled])
y_train_balanced = pd.concat([y_class_0, y_class_1_upsampled])

```

### パイプライン構築


```python
# パイプラインの構築
pipeline = Pipeline([
    ('clf', DecisionTreeClassifier(random_state=42))
])

# ハイパーパラメータグリッド
param_grid = {
    'clf__criterion': ['gini', 'entropy'],
    'clf__splitter': ['best', 'random'],
    'clf__max_depth': [3, 5, 10],
    'clf__min_samples_split': [2, 10, 20],
    'clf__min_samples_leaf': [1, 5, 10]
}

# グリッドサーチCVの設定
grid_search_dt = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)

```

### モデルのトレーニング


```python

# モデルのトレーニング
grid_search_dt.fit(X_train_balanced, y_train_balanced)

# 最適なパラメータ
best_params = grid_search_dt.best_params_
print("Best Parameters:", best_params)

```

    Best Parameters: {'clf__criterion': 'gini', 'clf__max_depth': 10, 'clf__min_samples_leaf': 1, 'clf__min_samples_split': 2, 'clf__splitter': 'best'}
    

### 混同行列


```python
# 最良のモデル
best_clf_dt = grid_search_dt.best_estimator_

# 最良のモデルによる予測値
y_pred_test = best_clf_dt.predict(X_test)

# モデルの評価
conf_matrix_dt = confusion_matrix(y_test, y_pred_test)
accuracy_dt = accuracy_score(y_test, y_pred_test)
report_dt = classification_report(y_test, y_pred_test, digits=3)

# ヒートマップに表示するための注釈を作成
labels = np.array([['TN={}'.format(conf_matrix_dt[0, 0]), 'FP={}'.format(conf_matrix_dt[0, 1])],
                ['FN={}'.format(conf_matrix_dt[1, 0]), 'TP={}'.format(conf_matrix_dt[1, 1])]])

# 混同行列をヒートマップとして表示
print('Decision Tree Confusion Matrix:')
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_dt, annot=labels, fmt='', cmap='Wistia', cbar=False, 
            xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'], 
            annot_kws={"color": "black", "fontsize": 16})
plt.xticks(fontsize=16)
plt.yticks(fontsize=16)
plt.xlabel('Predicted', fontsize=16)
plt.ylabel('Actual', fontsize=16)
plt.title('Decision Tree Confusion Matrix', fontsize=16)
# SVG形式で保存
confusion_matrix_svg_path = os.path.join(output_dir, "confusion_matrix.svg")
plt.savefig(confusion_matrix_svg_path, format='svg')
print(f"Confusion matrix saved to: {confusion_matrix_svg_path}")
plt.show()

```

    Decision Tree Confusion Matrix:
    Confusion matrix saved to: ./decision_tree_output/confusion_matrix.svg
    


    
![png](output_14_1.png)
    



```python
output_dir
```




    './decision_tree_output/'



### 性能評価指標


```python

# 性能評価を表示
print(f'Decision Tree Accuracy: {accuracy_dt:.3f}')

# print(conf_matrix_dt)
print('Decision Tree Classification Report:')
print(report_dt)

```

    Decision Tree Accuracy: 0.685
    Decision Tree Classification Report:
                  precision    recall  f1-score   support
    
               0      0.877     0.712     0.786      1627
               1      0.311     0.566     0.401       373
    
        accuracy                          0.685      2000
       macro avg      0.594     0.639     0.594      2000
    weighted avg      0.772     0.685     0.714      2000
    
    

### 特徴量の重要度


```python
# 特徴量名と重要度を取得
feature_importances = best_clf_dt.named_steps['clf'].feature_importances_
feature_names = X_train_balanced.columns

# 特徴量名と重要度を統一
importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': feature_importances
}).sort_values(by='Importance', ascending=False)

# 特徴量の重要度を棒グラフとして表示
plt.figure(figsize=(10, 6))
plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')
plt.xlabel('Importance', fontsize=14)
plt.ylabel('Feature', fontsize=14)
plt.title('Feature Importance from Decision Tree', fontsize=16)
plt.gca().invert_yaxis()
plt.tight_layout()

# 棒グラフをSVG形式で保存
feature_importance_svg = os.path.join(output_dir, 'feature_importance.svg')
plt.savefig(feature_importance_svg, format='svg')
plt.show()
```


    
![png](output_19_0.png)
    



```python
feature_names
```




    Index(['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7'], dtype='object')



### 木の深さと正解率


```python
# スコアを保存するリスト
evaluation_metrics = []
training_accuracies = []
test_accuracies = []

max_depth_range = range(1, 15)
for max_depth in max_depth_range:
    tree = DecisionTreeClassifier(criterion='gini', max_depth=max_depth, random_state=42)
    tree.fit(X_train_balanced, y_train_balanced)
    train_pred = tree.predict(X_train_balanced)
    test_pred = tree.predict(X_test)

    # トレーニングデータの精度を計算
    train_accuracy = accuracy_score(y_train_balanced, train_pred)
    training_accuracies.append(train_accuracy)

    # テストデータの精度を計算
    test_accuracy = accuracy_score(y_test, test_pred)
    test_accuracies.append(test_accuracy)

    # 精度、再現率、F1スコアを計算
    precision, recall, f1, _ = precision_recall_fscore_support(y_test, test_pred, average='weighted')
    evaluation_metrics.append({'max_depth': max_depth, 'accuracy': test_accuracy, 'precision': precision, 'recall': recall, 'f1_score': f1})

    # 決定木の可視化
    tree_dot = export_graphviz(tree, out_file=None, feature_names=X_train_balanced.columns, filled=True)
    tree_graph = graphviz.Source(tree_dot)
    tree_path = os.path.join(output_dir, f"decision_tree_depth_{max_depth}")
    tree_graph.render(tree_path, format="svg", cleanup=True)

    # 混同行列を計算
    cm = confusion_matrix(y_test, test_pred, labels=y.unique())
    fig, ax = plt.subplots(figsize=(6, 6))
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=y.unique())
    disp.plot(cmap="Greys", ax=ax, colorbar=False)
    plt.title(f"Confusion Matrix (Max Depth={max_depth})")
    plt.tight_layout()

    # 混同行列を保存
    confusion_matrix_path = os.path.join(output_dir, f"confusion_matrix_depth_{max_depth}.svg")
    plt.savefig(confusion_matrix_path, format="svg")
    plt.close(fig)

# トレーニングとテストの精度をプロット
plt.figure(figsize=(8, 5))
plt.plot(max_depth_range, training_accuracies, label="Training Accuracy", color="black", marker="o")
plt.plot(max_depth_range, test_accuracies, label="Test Accuracy", color="gray", linestyle="--", marker="o")
plt.title("Accuracy vs. Max Depth")
plt.xlabel("Max Depth")
plt.ylabel("Accuracy")
plt.legend()
plt.grid(True)
plt.tight_layout()

# 精度プロットをSVG形式で保存
accuracy_plot_path = os.path.join(output_dir, "accuracy_vs_max_depth.svg")
plt.savefig(accuracy_plot_path, format="svg")
plt.show()

# 精度評価をDataFrameに変換し保存
metrics_df = pd.DataFrame(evaluation_metrics)
metrics_path = os.path.join(output_dir, "evaluation_metrics.xlsx")
metrics_df.to_excel(metrics_path, index=False)
print(f"Evaluation metrics saved as Excel: {metrics_path}")

# 精度評価を表示
print("Evaluation Metrics by Max Depth:")
print(metrics_df)

```


    
![png](output_22_0.png)
    


    Evaluation metrics saved as Excel: ./decision_tree_output/evaluation_metrics.xlsx
    Evaluation Metrics by Max Depth:
        max_depth  accuracy  precision  recall  f1_score
    0           1    0.3440   0.798662  0.3440  0.348127
    1           2    0.7395   0.777043  0.7395  0.754926
    2           3    0.6475   0.787109  0.6475  0.687339
    3           4    0.6345   0.799043  0.6345  0.676629
    4           5    0.6460   0.791250  0.6460  0.686289
    5           6    0.6780   0.799234  0.6780  0.713349
    6           7    0.6640   0.793515  0.6640  0.701436
    7           8    0.6615   0.800576  0.6615  0.699791
    8           9    0.6775   0.796597  0.6775  0.712716
    9          10    0.6675   0.793839  0.6675  0.704335
    10         11    0.6825   0.785264  0.6825  0.715471
    11         12    0.6795   0.774955  0.6795  0.711697
    12         13    0.6850   0.771466  0.6850  0.715265
    13         14    0.6820   0.767100  0.6820  0.712238
    

### 分類木の可視化


```python
# 決定木のエクスポート
# モデル構造をDOT形式で出力
# 各特徴量やクラス名を視覚的に表示する設定
exported_dot = export_graphviz(
    best_clf_dt.named_steps['clf'],
    out_file=None,
    feature_names=X_train.columns,
    class_names=['Class 0', 'Class 1'],
    filled=True,
    rounded=True,
    special_characters=True
)

# Graphvizを使用した可視化
# DOT形式をGraphvizオブジェクトに変換
exported_graph = graphviz.Source(exported_dot)

# SVG形式でファイルに保存
exported_graph.render(os.path.join(output_dir, "decision_tree_visualization"), format='svg')
print(f"Decision tree exported to SVG format at {output_dir}")

# dtreevizによる高度な可視化
# 各特徴量の影響を視覚的に確認
visualized_tree = dtreeviz(
    best_clf_dt.named_steps['clf'],
    X_train_balanced,
    y_train_balanced,
    target_name='Target',
    feature_names=X_train.columns,
    class_names=['Class 0', 'Class 1']
)

# SVGファイルとして保存
viz_path = os.path.join(output_dir, "decision_tree_viz.svg")
visualized_tree.save(viz_path)
print(f"Decision tree visualization (dtreeviz) saved at {viz_path}")

# Jupyter Notebookなどで表示可能
visualized_tree
```

    Decision tree exported to SVG format at ./decision_tree_output/
    

    c:\anaconda3\Lib\site-packages\sklearn\base.py:439: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names
      warnings.warn(
    

    Decision tree visualization (dtreeviz) saved at ./decision_tree_output/decision_tree_viz.svg
    




    
![svg](output_24_3.svg)
    



## 参考　オーバーサンプリングなし


```python
import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier, export_graphviz
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import graphviz
from dtreeviz.trees import dtreeviz

# シードを固定して再現性を確保
np.random.seed(42)

# サンプルデータの生成
n_samples = 10000

# 特徴量とターゲットの生成
V1 = np.random.randint(20, 80, n_samples)
V2 = np.random.binomial(1, np.clip((V1 - 20) * 0.03, 0, 0.95))
V3 = np.random.choice([0, 1], n_samples)
V4 = np.random.choice([0, 1], n_samples, p=[0.7, 0.3])
V5 = np.random.choice([0, 1], n_samples, p=[0.8, 0.2])
V6 = np.random.choice([0, 1], n_samples, p=[0.7, 0.3])
V7 = np.random.choice([0, 1], n_samples, p=[0.9, 0.1])

# リスクスコアを計算する
# V1の値を基準とした基本スコアを計算
risk_score = np.clip((V1 - 20) * 0.005, 0, 0.2)

# 他の特徴量による影響を加味してスコアを調整
risk_score *= (1 + 2 * V4 + 0.5 * V5 + 1.5 * V6 + 2 * V7 + 0.1 * V3)

# V2の値によるリスク軽減効果を反映
risk_score *= (1 - 0.5 * V2)

# スコアを0から1の範囲に制限
risk_score = np.clip(risk_score, 0, 1)

# リスクスコアに基づいてターゲット値を決定
Target = np.random.binomial(1, risk_score)

# データフレームにまとめる
data = pd.DataFrame({
    'V1': V1,
    'V2': V2,
    'V3': V3,
    'V4': V4,
    'V5': V5,
    'V6': V6,
    'V7': V7,
    'Target': Target
})

# 出力ディレクトリ
output_dir = './decision_tree_output_no_oversampling/'
os.makedirs(output_dir, exist_ok=True)

# 特徴量とターゲットの分離
X = data.drop('Target', axis=1)
y = data['Target']

# データの分割
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# パイプラインの構築
pipeline = Pipeline([
    ('scaler', StandardScaler()),  # 標準化
    ('clf', DecisionTreeClassifier(random_state=42))
])

# ハイパーパラメータグリッド
param_grid = {
    'clf__criterion': ['gini', 'entropy'],
    'clf__splitter': ['best', 'random'],
    'clf__max_depth': [3, 5, 10],
    'clf__min_samples_split': [2, 10, 20],
    'clf__min_samples_leaf': [1, 5, 10]
}

# グリッドサーチCVの設定
grid_search_dt = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)

# モデルのトレーニング
grid_search_dt.fit(X_train, y_train)

# 最適なパラメータ
best_params = grid_search_dt.best_params_
print("Best Parameters:", best_params)

# 最良のモデル
best_clf_dt = grid_search_dt.best_estimator_

# テストデータに対する評価
y_pred_test = best_clf_dt.predict(X_test)

# 評価結果
best_accuracy = accuracy_score(y_test, y_pred_test)
best_report = classification_report(y_test, y_pred_test)
print("Accuracy on Test Data:", best_accuracy)
print("Classification Report on Test Data:\n", best_report)

# モデルの評価
accuracy_dt = accuracy_score(y_test, y_pred_test)
conf_matrix_dt = confusion_matrix(y_test, y_pred_test)
report_dt = classification_report(y_test, y_pred_test, digits=3)

# 結果を表示
print(f'Decision Tree Accuracy: {accuracy_dt:.3f}')
print('Decision Tree Classification Report:')
print(report_dt)

# 混同行列をヒートマップとして表示
print('Decision Tree Confusion Matrix:')
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_dt, annot=True, fmt='d', cmap='Wistia', cbar=False, 
            xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'], 
            annot_kws={"color": "black", "fontsize": 16})
plt.xticks(fontsize=16)
plt.yticks(fontsize=16)
plt.xlabel('Predicted', fontsize=16)
plt.ylabel('Actual', fontsize=16)
plt.title('Decision Tree Confusion Matrix', fontsize=16)
confusion_matrix_path = os.path.join(output_dir, "confusion_matrix_no_oversampling.svg")
plt.savefig(confusion_matrix_path, format='svg')
plt.show()

# 決定木のエクスポート
# モデル構造をDOT形式で出力
dot_data = export_graphviz(
    best_clf_dt.named_steps['clf'],
    out_file=None,
    feature_names=X_train.columns,
    class_names=['Class 0', 'Class 1'],
    filled=True,
    rounded=True,
    special_characters=True
)

# Graphvizを使用した可視化
# DOT形式をGraphvizオブジェクトに変換
graph = graphviz.Source(dot_data)

# SVG形式でファイルに保存
graph.render(os.path.join(output_dir, "decision_tree_visualization_no_oversampling"), format='svg')
print(f"Decision tree exported to SVG format at {output_dir}")

# dtreevizによる高度な可視化
# 各特徴量の影響を視覚的に確認
viz = dtreeviz(
    best_clf_dt.named_steps['clf'],
    X_train,
    y_train,
    target_name='Target',
    feature_names=X_train.columns,
    class_names=['Class 0', 'Class 1']
)

# SVGファイルとして保存
viz_path = os.path.join(output_dir, "decision_tree_viz_no_oversampling.svg")
viz.save(viz_path)
print(f"Decision tree visualization (dtreeviz) saved at {viz_path}")

# 可視化の表示
viz

```

    Best Parameters: {'clf__criterion': 'gini', 'clf__max_depth': 3, 'clf__min_samples_leaf': 1, 'clf__min_samples_split': 2, 'clf__splitter': 'random'}
    Accuracy on Test Data: 0.8245
    Classification Report on Test Data:
                   precision    recall  f1-score   support
    
               0       0.83      0.99      0.90      1639
               1       0.68      0.05      0.10       361
    
        accuracy                           0.82      2000
       macro avg       0.75      0.52      0.50      2000
    weighted avg       0.80      0.82      0.76      2000
    
    Decision Tree Accuracy: 0.825
    Decision Tree Classification Report:
                  precision    recall  f1-score   support
    
               0      0.827     0.995     0.903      1639
               1      0.679     0.053     0.098       361
    
        accuracy                          0.825      2000
       macro avg      0.753     0.524     0.500      2000
    weighted avg      0.800     0.825     0.757      2000
    
    Decision Tree Confusion Matrix:
    


    
![png](output_26_1.png)
    


    Decision tree exported to SVG format at ./decision_tree_output_no_oversampling/
    Decision tree visualization (dtreeviz) saved at ./decision_tree_output_no_oversampling/decision_tree_viz_no_oversampling.svg
    




    
![svg](output_26_3.svg)
    



## LightGBMモデル


```python
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.utils import resample
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report
import pandas as pd
import lightgbm as lgb

# データの読み込み
data = pd.read_csv('./decision_tree_output/generated_data_fixed.csv')  # ファイルパスを修正してください

# AgeGroup列を削除
data = data.drop(columns=['AgeGroup'])

# 特徴量とターゲットの分離
X = data.drop('Severe', axis=1)
y = data['Severe']

# データの分割
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 訓練用データで少数クラスのオーバーサンプリング
class_0 = X_train[y_train == 0]
class_1 = X_train[y_train == 1]
y_class_0 = y_train[y_train == 0]
y_class_1 = y_train[y_train == 1]

X_class_1_upsampled, y_class_1_upsampled = resample(class_1,
                                                    y_class_1,
                                                    replace=True,  # サンプルを複製
                                                    n_samples=len(class_0),  # クラス0と同じ数にする
                                                    random_state=42)  # 乱数シード

X_train_balanced = pd.concat([class_0, X_class_1_upsampled])
y_train_balanced = pd.concat([y_class_0, y_class_1_upsampled])

# パイプラインの構築
pipeline = Pipeline([
    ('scaler', StandardScaler()),  # 標準化
    ('clf', lgb.LGBMClassifier(random_state=42))
])

# ハイパーパラメータグリッド
param_grid = {
    'clf__num_leaves': [31, 50, 100],
    'clf__learning_rate': [0.1, 0.01, 0.001],
    'clf__n_estimators': [100, 200, 500]
}

# グリッドサーチCVの設定
grid_search_lgb = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)

# モデルのトレーニング
grid_search_lgb.fit(X_train_balanced, y_train_balanced)

# 最適なパラメータ
best_params = grid_search_lgb.best_params_

# 最良のモデル
best_clf_lgb = grid_search_lgb.best_estimator_

# テストデータに対する評価
y_pred_test = best_clf_lgb.predict(X_test)

# 評価結果
best_accuracy = accuracy_score(y_test, y_pred_test)
best_report = classification_report(y_test, y_pred_test)

print("Best Parameters:", best_params)
print("Accuracy on Test Data:", best_accuracy)
print("Classification Report on Test Data:\n", best_report)


# from sklearn.model_selection import train_test_split, GridSearchCV
# import lightgbm as lgb


# # 特徴量とターゲットに分ける
# X = df[['Age', 'Vaccinated', 'Gender', 'Comorbidities', 'Smoking', 'Obesity', 'Hypoxia']]
# y = df['Severe']

# # データをトレーニングセットとテストセットに分ける
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# # LightGBMモデルのハイパーパラメータチューニング
# param_grid = {
#     'num_leaves': [31, 50, 70],
#     'max_depth': [-1, 10, 20],
#     'learning_rate': [0.01, 0.1, 0.2],
#     'n_estimators': [100, 200, 500]
# }
# clf_lgb = lgb.LGBMClassifier(random_state=42)
# grid_search_lgb = GridSearchCV(clf_lgb, param_grid, scoring='f1', cv=5)
# grid_search_lgb.fit(X_train, y_train)

# # 最良モデルで予測
# best_clf_lgb = grid_search_lgb.best_estimator_
# y_pred_lgb = best_clf_lgb.predict(X_test)

# # モデルの評価
# accuracy_lgb = accuracy_score(y_test, y_pred_lgb)
# conf_matrix_lgb = confusion_matrix(y_test, y_pred_lgb)
# report_lgb = classification_report(y_test, y_pred_lgb)

# # 結果を表示
# print(f'LightGBM Accuracy: {accuracy_lgb:.2f}')
# print('LightGBM Confusion Matrix:')
# print(conf_matrix_lgb)
# print('LightGBM Classification Report:')
# print(report_lgb)

# # 混同行列をヒートマップとして表示
# plt.figure(figsize=(8, 6))
# sns.heatmap(conf_matrix_lgb, annot=True, fmt='d', cmap='viridis', cbar=False, xticklabels=['Non-Severe', 'Severe'], yticklabels=['Non-Severe', 'Severe'], annot_kws={"color": "black"})
# plt.xlabel('Predicted')
# plt.ylabel('Actual')
# plt.title('LightGBM Confusion Matrix')
# plt.show()

# # # 予測結果と実際の重症化率を比較する散布図
# # df_test_lgb = X_test.copy()
# # df_test_lgb['TrueSeverity'] = y_test
# # df_test_lgb['PredictedSeverity'] = y_pred_lgb

# # plt.figure(figsize=(12, 6))
# # sns.scatterplot(x='Age', y='TrueSeverity', data=df_test_lgb, hue='Vaccinated', style='PredictedSeverity', palette='viridis', markers=['o', 'X'])
# # plt.title('True vs Predicted Severity by Age and Vaccination Status (LightGBM)')
# # plt.xlabel('Age')
# # plt.ylabel('Severity (0: Non-Severe, 1: Severe)')
# # plt.legend(title='Vaccinated', loc='upper left', labels=['Not Vaccinated, Predicted', 'Not Vaccinated, True', 'Vaccinated, Predicted', 'Vaccinated, True'])
# # plt.show()

```

    [LightGBM] [Info] Number of positive: 6542, number of negative: 6542
    [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000107 seconds.
    You can set `force_row_wise=true` to remove the overhead.
    And if memory is not enough, you can set `force_col_wise=true`.
    [LightGBM] [Info] Total Bins 79
    [LightGBM] [Info] Number of data points in the train set: 13084, number of used features: 7
    [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
    Best Parameters: {'clf__learning_rate': 0.1, 'clf__n_estimators': 500, 'clf__num_leaves': 100}
    Accuracy on Test Data: 0.6755
    Classification Report on Test Data:
                   precision    recall  f1-score   support
    
               0       0.87      0.71      0.78      1639
               1       0.28      0.51      0.36       361
    
        accuracy                           0.68      2000
       macro avg       0.57      0.61      0.57      2000
    weighted avg       0.76      0.68      0.71      2000
    
    


```python
# モデルの評価
accuracy_dt = accuracy_score(y_test, y_pred_test)
conf_matrix_dt = confusion_matrix(y_test, y_pred_test)
report_dt = classification_report(y_test, y_pred_test, digits=3)

# 結果を表示
print(f'Decision Tree Accuracy: {accuracy_dt:.3f}')

# print(conf_matrix_dt)
print('Decision Tree Classification Report:')
print(report_dt)



# 混同行列をヒートマップとして表示
print('Decision Tree Confusion Matrix:')
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_dt, annot=True, fmt='d', cmap='Wistia', cbar=False, xticklabels=['Non-Severe', 'Severe'], yticklabels=['Non-Severe', 'Severe'], annot_kws={"color": "black", "fontsize": 16})
plt.xticks(fontsize=16)
plt.yticks(fontsize=16)
plt.xlabel('Predicted', fontsize=16)
plt.ylabel('Actual', fontsize=16)
plt.title('Decision Tree Confusion Matrix', fontsize=16)
plt.show()
```

    Decision Tree Accuracy: 0.675
    Decision Tree Classification Report:
                  precision    recall  f1-score   support
    
               0      0.868     0.713     0.783      1639
               1      0.280     0.507     0.361       361
    
        accuracy                          0.675      2000
       macro avg      0.574     0.610     0.572      2000
    weighted avg      0.762     0.675     0.706      2000
    
    Decision Tree Confusion Matrix:
    


    
![png](output_29_1.png)
    



```python
# オーバーサンプリングなし

from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report
import pandas as pd
import lightgbm as lgb

# データの読み込み
data = pd.read_csv('./decision_tree_output/generated_data_fixed.csv')  # ファイルパスを修正してください

# AgeGroup列を削除
data = data.drop(columns=['AgeGroup'])

# 特徴量とターゲットの分離
X = data.drop('Severe', axis=1)
y = data['Severe']

# データの分割
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# パイプラインの構築
pipeline = Pipeline([
    ('scaler', StandardScaler()),  # 標準化
    ('clf', lgb.LGBMClassifier(random_state=42))
])

# ハイパーパラメータグリッド
param_grid = {
    'clf__num_leaves': [31, 50, 100],
    'clf__learning_rate': [0.1, 0.01, 0.001],
    'clf__n_estimators': [100, 200, 500]
}

# グリッドサーチCVの設定
grid_search_lgb = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)

# モデルのトレーニング
grid_search_lgb.fit(X_train, y_train)

# 最適なパラメータ
best_params = grid_search_lgb.best_params_

# 最良のモデル
best_clf_lgb = grid_search_lgb.best_estimator_

# テストデータに対する評価
y_pred_test = best_clf_lgb.predict(X_test)

# 評価結果
best_accuracy = accuracy_score(y_test, y_pred_test)
best_report = classification_report(y_test, y_pred_test)

print("Best Parameters:", best_params)
print("Accuracy on Test Data:", best_accuracy)
print("Classification Report on Test Data:\n", best_report)

```

    [LightGBM] [Info] Number of positive: 1458, number of negative: 6542
    [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000590 seconds.
    You can set `force_row_wise=true` to remove the overhead.
    And if memory is not enough, you can set `force_col_wise=true`.
    [LightGBM] [Info] Total Bins 79
    [LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7
    [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.182250 -> initscore=-1.501177
    [LightGBM] [Info] Start training from score -1.501177
    Best Parameters: {'clf__learning_rate': 0.01, 'clf__n_estimators': 200, 'clf__num_leaves': 50}
    Accuracy on Test Data: 0.822
    Classification Report on Test Data:
                   precision    recall  f1-score   support
    
               0       0.83      0.99      0.90      1639
               1       0.56      0.07      0.12       361
    
        accuracy                           0.82      2000
       macro avg       0.69      0.53      0.51      2000
    weighted avg       0.78      0.82      0.76      2000
    
    


```python
# モデルの評価
accuracy_dt = accuracy_score(y_test, y_pred_test)
conf_matrix_dt = confusion_matrix(y_test, y_pred_test)
report_dt = classification_report(y_test, y_pred_test, digits=3)

# 結果を表示
print(f'Decision Tree Accuracy: {accuracy_dt:.3f}')

# print(conf_matrix_dt)
print('Decision Tree Classification Report:')
print(report_dt)



# 混同行列をヒートマップとして表示
print('Decision Tree Confusion Matrix:')
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_dt, annot=True, fmt='d', cmap='Wistia', cbar=False, xticklabels=['Non-Severe', 'Severe'], yticklabels=['Non-Severe', 'Severe'], annot_kws={"color": "black", "fontsize": 16})
plt.xticks(fontsize=16)
plt.yticks(fontsize=16)
plt.xlabel('Predicted', fontsize=16)
plt.ylabel('Actual', fontsize=16)
plt.title('Decision Tree Confusion Matrix', fontsize=16)
plt.show()
```

    Decision Tree Accuracy: 0.822
    Decision Tree Classification Report:
                  precision    recall  f1-score   support
    
               0      0.828     0.988     0.901      1639
               1      0.558     0.066     0.119       361
    
        accuracy                          0.822      2000
       macro avg      0.693     0.527     0.510      2000
    weighted avg      0.779     0.822     0.760      2000
    
    Decision Tree Confusion Matrix:
    


    
![png](output_31_1.png)
    


## XGBoost


```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score
import xgboost as xgb

# シードを固定して再現性を確保
np.random.seed(42)

# サンプルデータの生成
n_samples = 10000

# 年齢を20歳から80歳の範囲でランダムに生成
ages = np.random.randint(20, 80, n_samples)

# 年齢に基づいてワクチン接種の確率を設定（高齢者の方がワクチン接種率が非常に高い）
vaccination_prob = np.clip((ages - 20) * 0.03, 0, 0.95)
vaccinated = np.random.binomial(1, vaccination_prob)

# 性別をランダムに生成 (0: 女性, 1: 男性)
gender = np.random.choice([0, 1], n_samples)

# 基礎疾患の有無をランダムに生成 (0: 無し, 1: 有り)
comorbidities = np.random.choice([0, 1], n_samples, p=[0.7, 0.3])

# 喫煙の有無をランダムに生成 (0: 非喫煙者, 1: 喫煙者)
smoking = np.random.choice([0, 1], n_samples, p=[0.8, 0.2])

# 肥満の有無をランダムに生成 (0: 非肥満, 1: 肥満)
obesity = np.random.choice([0, 1], n_samples, p=[0.7, 0.3])

# 酸素飽和度 (低酸素血症をランダムに生成, 0: 正常, 1: 低酸素血症)
hypoxia = np.random.choice([0, 1], n_samples, p=[0.9, 0.1])

# 年齢に基づいて重症化率を設定
severity_rate_by_age = np.clip((ages - 20) * 0.01, 0, 0.5)

# 基礎疾患、喫煙、肥満、低酸素血症、性別の影響を加味した重症化率
severity_rate = severity_rate_by_age * (
    1 - 0.8 * (1 - comorbidities) + 0.8 * comorbidities + 
    1 - 0.5 * (1 - smoking) + 0.5 * smoking + 
    1 - 0.8 * (1 - obesity) + 0.8 * obesity + 
    1 - 0.5 * (1 - hypoxia) + 0.5 * hypoxia + 
    0.2 * gender
)

# ワクチン接種の効果を考慮した重症化率の調整
severity_rate *= (1 - 0.5 * vaccinated)

# 重症化率を0から1の範囲にクリップ
severity_rate = np.clip(severity_rate, 0, 1)

# 重症化したかどうかをランダムに決定
severe_cases = np.random.binomial(1, severity_rate)

# データフレームにまとめる
df = pd.DataFrame({
    'Age': ages,
    'Vaccinated': vaccinated,
    'Gender': gender,
    'Comorbidities': comorbidities,
    'Smoking': smoking,
    'Obesity': obesity,
    'Hypoxia': hypoxia,
    'Severe': severe_cases
})

# データの可視化
plt.figure(figsize=(12, 6))
sns.histplot(data=df, x='Age', hue='Severe', multiple='stack', palette='viridis')
plt.title('Age Distribution by Severity')
plt.show()

# ワクチン接種の有無と重症化率の関係を年齢層を無視して可視化
severity_rate_by_vaccination = df.groupby('Vaccinated')['Severe'].mean().reset_index()
plt.figure(figsize=(8, 6))
sns.barplot(x='Vaccinated', y='Severe', data=severity_rate_by_vaccination, palette='viridis')
plt.title('Severity Rate by Vaccination Status (Without Age Stratification)')
plt.show()

# 年齢層ごとのワクチン接種の有無と重症化率の関係を可視化
df['AgeGroup'] = pd.cut(df['Age'], bins=[20, 30, 40, 50, 60, 70, 80], labels=['20-30', '30-40', '40-50', '50-60', '60-70', '70-80'])
severity_rate_by_age_vaccination = df.groupby(['AgeGroup', 'Vaccinated'])['Severe'].mean().reset_index()
plt.figure(figsize=(12, 6))
sns.lineplot(x='AgeGroup', y='Severe', hue='Vaccinated', data=severity_rate_by_age_vaccination, marker='o', palette='viridis')
plt.title('Severity Rate by Age Group and Vaccination Status')
plt.show()

# クロス集計表の作成
cross_tab = pd.crosstab(df['AgeGroup'], df['Vaccinated'], values=df['Severe'], aggfunc='mean')
print("Cross Tabulation of Severity Rate by Age Group and Vaccination Status:")
print(cross_tab)

# 特徴量とターゲットに分ける
X = df[['Age', 'Vaccinated', 'Gender', 'Comorbidities', 'Smoking', 'Obesity', 'Hypoxia']]
y = df['Severe']

# データをトレーニングセットとテストセットに分ける
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# XGBoostモデルのハイパーパラメータチューニング
param_grid = {
    'max_depth': [3, 5, 7, 10],
    'learning_rate': [0.01, 0.1, 0.2],
    'n_estimators': [100, 200, 500],
    'subsample': [0.8, 1.0]
}
clf_xgb = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')
grid_search_xgb = GridSearchCV(clf_xgb, param_grid, scoring='f1', cv=5)
grid_search_xgb.fit(X_train, y_train)

# 最良モデルで予測
best_clf_xgb = grid_search_xgb.best_estimator_
y_pred_xgb = best_clf_xgb.predict(X_test)

# モデルの評価
accuracy_xgb = accuracy_score(y_test, y_pred_xgb)
conf_matrix_xgb = confusion_matrix(y_test, y_pred_xgb)
report_xgb = classification_report(y_test, y_pred_xgb)

# 結果を表示
print(f'XGBoost Accuracy: {accuracy_xgb:.2f}')
print('XGBoost Confusion Matrix:')
print(conf_matrix_xgb)
print('XGBoost Classification Report:')
print(report_xgb)

# 混同行列をヒートマップとして表示
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_xgb, annot=True, fmt='d', cmap='viridis', cbar=False, xticklabels=['Non-Severe', 'Severe'], yticklabels=['Non-Severe', 'Severe'], annot_kws={"color": "black"})
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('XGBoost Confusion Matrix')
plt.show()

# 予測結果と実際の重症化率を比較する散布図
df_test_xgb = X_test.copy()
df_test_xgb['TrueSeverity'] = y_test
df_test_xgb['PredictedSeverity'] = y_pred_xgb

plt.figure(figsize=(12, 6))
sns.scatterplot(x='Age', y='TrueSeverity', data=df_test_xgb, hue='Vaccinated', style='PredictedSeverity', palette='viridis', markers=['o', 'X'])
plt.title('True vs Predicted Severity by Age and Vaccination Status (XGBoost)')
plt.xlabel('Age')
plt.ylabel('Severity (0: Non-Severe, 1: Severe)')
plt.legend(title='Vaccinated', loc='upper left', labels=['Not Vaccinated, Predicted', 'Not Vaccinated, True', 'Vaccinated, Predicted', 'Vaccinated, True'])
plt.show()
```


    
![png](output_33_0.png)
    


    C:\Users\tamta\AppData\Local\Temp\ipykernel_28192\3625012845.py:79: FutureWarning: 
    
    Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.
    
      sns.barplot(x='Vaccinated', y='Severe', data=severity_rate_by_vaccination, palette='viridis')
    


    
![png](output_33_2.png)
    



    
![png](output_33_3.png)
    


    Cross Tabulation of Severity Rate by Age Group and Vaccination Status:
    Vaccinated         0         1
    AgeGroup                      
    20-30       0.155050  0.081181
    30-40       0.432609  0.227794
    40-50       0.641711  0.378522
    50-60       0.804124  0.476730
    60-70       0.924051  0.609288
    70-80       0.950820  0.664275
    XGBoost Accuracy: 0.74
    XGBoost Confusion Matrix:
    [[1380  296]
     [ 480  844]]
    XGBoost Classification Report:
                  precision    recall  f1-score   support
    
               0       0.74      0.82      0.78      1676
               1       0.74      0.64      0.69      1324
    
        accuracy                           0.74      3000
       macro avg       0.74      0.73      0.73      3000
    weighted avg       0.74      0.74      0.74      3000
    
    


    
![png](output_33_5.png)
    



    
![png](output_33_6.png)
    


## 各要因と重症化率の関係(年齢で層化）


```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 生成したデータを再利用
# (この部分は以前のコードと同じです)
np.random.seed(42)
n_samples = 10000
ages = np.random.randint(20, 80, n_samples)
vaccination_prob = np.clip((ages - 20) * 0.03, 0, 0.98)
vaccinated = np.random.binomial(1, vaccination_prob)
gender = np.random.choice([0, 1], n_samples)
comorbidities = np.random.choice([0, 1], n_samples, p=[0.7, 0.3])
smoking = np.random.choice([0, 1], n_samples, p=[0.8, 0.2])
obesity = np.random.choice([0, 1], n_samples, p=[0.7, 0.3])
hypoxia = np.random.choice([0, 1], n_samples, p=[0.9, 0.1])
severity_rate_by_age = np.clip((ages - 20) * 0.01, 0, 0.5)
severity_rate = severity_rate_by_age * (
    1 - 0.8 * (1 - comorbidities) + 0.8 * comorbidities + 
    1 - 0.5 * (1 - smoking) + 0.5 * smoking + 
    1 - 0.8 * (1 - obesity) + 0.8 * obesity + 
    1 - 0.5 * (1 - hypoxia) + 0.5 * hypoxia + 
    0.2 * gender
)
severity_rate *= (1 - 0.5 * vaccinated)
severity_rate = np.clip(severity_rate, 0, 1)
severe_cases = np.random.binomial(1, severity_rate)

df = pd.DataFrame({
    'Age': ages,  # 年齢
    'Vaccinated': vaccinated,  # ワクチン接種の有無
    'Gender': gender,  # 性別
    'Comorbidities': comorbidities,  # 基礎疾患の有無
    'Smoking': smoking,  # 喫煙の有無
    'Obesity': obesity,  # 肥満の有無
    'Hypoxia': hypoxia,  # 低酸素血症の有無
    'Severe': severe_cases  # 重症化の有無
})

# 年齢層ごとにデータを層化
df['AgeGroup'] = pd.cut(df['Age'], bins=[20, 30, 40, 50, 60, 70, 80], labels=['20-30', '30-40', '40-50', '50-60', '60-70', '70-80'])

# 各要因と重症化率の関係を可視化
factors = ['Comorbidities', 'Smoking', 'Obesity', 'Hypoxia', 'Gender']

for factor in factors:
    plt.figure(figsize=(12, 6))
    factor_severity_rate = df.groupby(['AgeGroup', factor])['Severe'].mean().reset_index()
    sns.barplot(x='AgeGroup', y='Severe', hue=factor, data=factor_severity_rate, palette='viridis')
    plt.title(f'Severity Rate by {factor.capitalize()} and Age Group')
    plt.ylabel('Severity Rate')
    plt.xlabel('Age Group')
    plt.show()

```


    
![png](output_35_0.png)
    



    
![png](output_35_1.png)
    



    
![png](output_35_2.png)
    



    
![png](output_35_3.png)
    



    
![png](output_35_4.png)
    

