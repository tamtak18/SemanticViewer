# 1. スクレイピング

個人打撃成績


```python
import os
import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime

# 保存フォルダのパス設定
output_dir = "./プロ野球データ打撃成績"
os.makedirs(output_dir, exist_ok=True)  # フォルダが存在しない場合は作成

# データを格納するリスト
all_data = []

# 2012年から2023年までのデータを取得
for year in range(2012, 2024):
    print(f"{year}年のデータを取得中...")
    
    # URLリストを設定
    urls = [
        f"https://baseballdata.jp/{year}/ptop.html",
        f"https://baseballdata.jp/{year}/ctop.html",
        f"https://baseballdata.jp/{year}/pdrm.html",
        f"https://baseballdata.jp/{year}/cdrm.html"
    ]
    
    for url in urls:
        # HTMLの取得
        response = requests.get(url)
        response.encoding = response.apparent_encoding  # ページのエンコーディングに基づいてデコード
        html = response.text

        # BeautifulSoupでHTMLをパース
        soup = BeautifulSoup(html, 'html.parser')

        # 打撃成績のテーブルを抽出
        table = soup.find("table", {"class": "table-sticky"})
        if table is None:
            print(f"{url}のデータが見つかりません．スキップします．")
            continue

        # ヘッダーとデータの抽出
        headers = [th.get_text(strip=True) for th in table.find("thead").find_all("th")]
        rows = []
        for row in table.find("tbody").find_all("tr"):
            cells = [cell.get_text(strip=True) for cell in row.find_all(["th", "td"])]
            if len(cells) == len(headers):  # ヘッダーの長さに一致する行のみ処理
                rows.append(cells)

        # データフレームに変換
        df = pd.DataFrame(rows, columns=headers)

        # 二重ヘッダーの上部行を削除
        df = df[~df['選手名'].str.contains('選手名', na=False)]

        # 選手名の「数字:」を削除
        df['選手名'] = df['選手名'].str.replace(r'^\d+:', '', regex=True)

        # 選手名の半角および全角スペースを削除
        df['選手名'] = df['選手名'].str.replace(r'\s+', '', regex=True)

        # 年度カラムを追加
        df.insert(loc=df.columns.get_loc('選手名') + 1, column='年度', value=year)

        # 調子カラムを削除
        if '調子' in df.columns:
            df = df.drop(columns=['調子'])

        # 取得したデータをリストに追加
        all_data.append(df)

# 全データを結合
final_df = pd.concat(all_data, ignore_index=True)

# 選手名と年度でソート
final_df = final_df.sort_values(by=['選手名', '年度']).reset_index(drop=True)

# ファイルパス設定
# 現在の時刻を取得し、指定の形式で文字列に変換
timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
output_file = os.path.join(output_dir, f"batting_stats_2012_2023_{timestamp}.csv")

# データフレームを保存
final_df.to_csv(output_file, index=False, encoding="utf-8-sig")

print(f"すべてのデータが {output_file} に保存されました．")

```

    2012年のデータを取得中...
    2013年のデータを取得中...
    2014年のデータを取得中...
    2015年のデータを取得中...
    2016年のデータを取得中...
    2017年のデータを取得中...
    2018年のデータを取得中...
    2019年のデータを取得中...
    2020年のデータを取得中...
    2021年のデータを取得中...
    2022年のデータを取得中...
    2023年のデータを取得中...
    すべてのデータが ./プロ野球データ打撃成績\batting_stats_2012_2023.csv に保存されました．
    

# 2. 前処理

## 翌年OPS&本塁打カラム作成


```python
import pandas as pd

# ファイルパスの設定
input_file_path = "./プロ野球データ打撃成績/batting_stats_2012_2023.csv"
output_file_path = "./プロ野球データ打撃成績/batting_stats_昨年翌年OPS本塁打_2012_2023.xlsx"

# データの読み込み
df = pd.read_csv(input_file_path, encoding="utf-8-sig")

# カラムを追加する処理
df["翌年OPS"] = None  # 空の「翌年OPS」カラムを追加
df["昨年OPS"] = None  
df["翌年本塁打"] = None  # 空の「翌年本塁打」カラムを追加
df["昨年本塁打"] = None


# 各選手の翌年本塁打を計算して挿入
for index, row in df.iterrows():
    # 現在の選手名と年度を取得
    player_name = row["選手名"]
    year = row["年度"]

    # 翌年のデータを検索
    next_year_data = df[(df["選手名"] == player_name) & (df["年度"] == year + 1)]

    # 翌年本塁打が存在する場合は値を代入
    if not next_year_data.empty:
        df.at[index, "翌年OPS"] = next_year_data.iloc[0]["OPS"]
        df.at[index, "翌年本塁打"] = next_year_data.iloc[0]["本塁打"]

    # 昨年のデータを検索
    previous_year_data = df[(df["選手名"] == player_name) & (df["年度"] == year - 1)]

    # 昨年本塁打が存在する場合は値を代入
    if not previous_year_data.empty:
        df.at[index, "昨年OPS"] = previous_year_data.iloc[0]["OPS"]
        df.at[index, "昨年本塁打"] = previous_year_data.iloc[0]["本塁打"]

# 欠損値を0埋め
df.fillna(0, inplace=True)

# 「.---」を「.000」に変換
df.replace(".---", ".000", inplace=True)

# xlsx形式として保存
df.to_excel(output_file_path, index=False, engine="openpyxl")

```

## データ理解


```python
import pandas as pd
import numpy as np
import matplotlib.font_manager as fm

# データの読み込みとチーム名の変換
file_path = './プロ野球データ打撃成績/batting_stats_昨年翌年OPS本塁打selected_2012_2023.xlsx'
data = pd.read_excel(file_path)

# 1. 基本的な情報の確認を表にする
info_df = pd.DataFrame({
    "Column": data.columns,
    "Non-Null Count": [data[col].notnull().sum() for col in data.columns],
    "Dtype": [data[col].dtype for col in data.columns]
})

# 2. 統計量の確認
describe_df = data.describe()

# 表示
print("基本情報:")
display(info_df)

print("\n統計量:")
display(describe_df)

```

    基本情報:
    


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Column</th>
      <th>Non-Null Count</th>
      <th>Dtype</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>選手名</td>
      <td>6774</td>
      <td>object</td>
    </tr>
    <tr>
      <th>1</th>
      <td>年度</td>
      <td>6774</td>
      <td>int64</td>
    </tr>
    <tr>
      <th>2</th>
      <td>球団</td>
      <td>6774</td>
      <td>object</td>
    </tr>
    <tr>
      <th>3</th>
      <td>打率</td>
      <td>6774</td>
      <td>float64</td>
    </tr>
    <tr>
      <th>4</th>
      <td>打点</td>
      <td>6774</td>
      <td>int64</td>
    </tr>
    <tr>
      <th>5</th>
      <td>昨年本塁打</td>
      <td>6774</td>
      <td>int64</td>
    </tr>
    <tr>
      <th>6</th>
      <td>本塁打</td>
      <td>6774</td>
      <td>int64</td>
    </tr>
    <tr>
      <th>7</th>
      <td>安打数</td>
      <td>6774</td>
      <td>int64</td>
    </tr>
    <tr>
      <th>8</th>
      <td>単打</td>
      <td>6774</td>
      <td>int64</td>
    </tr>
    <tr>
      <th>9</th>
      <td>2塁打</td>
      <td>6774</td>
      <td>int64</td>
    </tr>
    <tr>
      <th>10</th>
      <td>3塁打</td>
      <td>6774</td>
      <td>int64</td>
    </tr>
    <tr>
      <th>11</th>
      <td>出塁率</td>
      <td>6774</td>
      <td>float64</td>
    </tr>
    <tr>
      <th>12</th>
      <td>長打率</td>
      <td>6774</td>
      <td>float64</td>
    </tr>
    <tr>
      <th>13</th>
      <td>昨年OPS</td>
      <td>6774</td>
      <td>float64</td>
    </tr>
    <tr>
      <th>14</th>
      <td>OPS</td>
      <td>6774</td>
      <td>float64</td>
    </tr>
    <tr>
      <th>15</th>
      <td>得点圏打数</td>
      <td>6774</td>
      <td>int64</td>
    </tr>
    <tr>
      <th>16</th>
      <td>得点圏安打</td>
      <td>6774</td>
      <td>int64</td>
    </tr>
    <tr>
      <th>17</th>
      <td>得点圏打率</td>
      <td>6774</td>
      <td>float64</td>
    </tr>
    <tr>
      <th>18</th>
      <td>試合数</td>
      <td>6774</td>
      <td>int64</td>
    </tr>
    <tr>
      <th>19</th>
      <td>打席数</td>
      <td>6774</td>
      <td>int64</td>
    </tr>
    <tr>
      <th>20</th>
      <td>打数</td>
      <td>6774</td>
      <td>int64</td>
    </tr>
    <tr>
      <th>21</th>
      <td>得点</td>
      <td>6774</td>
      <td>int64</td>
    </tr>
    <tr>
      <th>22</th>
      <td>四球</td>
      <td>6774</td>
      <td>int64</td>
    </tr>
    <tr>
      <th>23</th>
      <td>死球</td>
      <td>6774</td>
      <td>int64</td>
    </tr>
    <tr>
      <th>24</th>
      <td>犠飛</td>
      <td>6774</td>
      <td>int64</td>
    </tr>
    <tr>
      <th>25</th>
      <td>併殺</td>
      <td>6774</td>
      <td>int64</td>
    </tr>
    <tr>
      <th>26</th>
      <td>三振</td>
      <td>6774</td>
      <td>int64</td>
    </tr>
    <tr>
      <th>27</th>
      <td>翌年本塁打</td>
      <td>6774</td>
      <td>int64</td>
    </tr>
  </tbody>
</table>
</div>


    
    統計量:
    


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>年度</th>
      <th>打率</th>
      <th>打点</th>
      <th>昨年本塁打</th>
      <th>本塁打</th>
      <th>安打数</th>
      <th>単打</th>
      <th>2塁打</th>
      <th>3塁打</th>
      <th>出塁率</th>
      <th>...</th>
      <th>試合数</th>
      <th>打席数</th>
      <th>打数</th>
      <th>得点</th>
      <th>四球</th>
      <th>死球</th>
      <th>犠飛</th>
      <th>併殺</th>
      <th>三振</th>
      <th>翌年本塁打</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>6774.000000</td>
      <td>6774.000000</td>
      <td>6774.000000</td>
      <td>6774.000000</td>
      <td>6774.000000</td>
      <td>6774.000000</td>
      <td>6774.000000</td>
      <td>6774.000000</td>
      <td>6774.000000</td>
      <td>6774.000000</td>
      <td>...</td>
      <td>6774.000000</td>
      <td>6774.000000</td>
      <td>6774.000000</td>
      <td>6774.000000</td>
      <td>6774.000000</td>
      <td>6774.000000</td>
      <td>6774.000000</td>
      <td>6774.000000</td>
      <td>6774.000000</td>
      <td>6774.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>2017.444789</td>
      <td>0.144678</td>
      <td>11.091526</td>
      <td>2.040006</td>
      <td>2.391202</td>
      <td>25.100827</td>
      <td>17.990847</td>
      <td>4.272365</td>
      <td>0.446413</td>
      <td>0.186327</td>
      <td>...</td>
      <td>41.879392</td>
      <td>113.065988</td>
      <td>99.789637</td>
      <td>11.620756</td>
      <td>9.389578</td>
      <td>1.046058</td>
      <td>0.646147</td>
      <td>2.034544</td>
      <td>21.286094</td>
      <td>2.001771</td>
    </tr>
    <tr>
      <th>std</th>
      <td>3.485450</td>
      <td>0.135710</td>
      <td>20.520034</td>
      <td>5.543037</td>
      <td>5.799460</td>
      <td>41.914194</td>
      <td>29.862427</td>
      <td>7.598659</td>
      <td>1.162435</td>
      <td>0.162507</td>
      <td>...</td>
      <td>42.105873</td>
      <td>171.936143</td>
      <td>151.307687</td>
      <td>20.136771</td>
      <td>17.487144</td>
      <td>2.233975</td>
      <td>1.369276</td>
      <td>3.701068</td>
      <td>30.232783</td>
      <td>5.498882</td>
    </tr>
    <tr>
      <th>min</th>
      <td>2012.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>2014.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>...</td>
      <td>8.000000</td>
      <td>2.000000</td>
      <td>2.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>2017.000000</td>
      <td>0.167000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>3.000000</td>
      <td>2.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.216000</td>
      <td>...</td>
      <td>24.000000</td>
      <td>25.000000</td>
      <td>22.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>7.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>2021.000000</td>
      <td>0.247000</td>
      <td>12.000000</td>
      <td>1.000000</td>
      <td>2.000000</td>
      <td>30.000000</td>
      <td>22.000000</td>
      <td>5.000000</td>
      <td>0.000000</td>
      <td>0.311000</td>
      <td>...</td>
      <td>65.000000</td>
      <td>148.750000</td>
      <td>131.000000</td>
      <td>14.000000</td>
      <td>10.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>2.000000</td>
      <td>29.000000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>2023.000000</td>
      <td>1.000000</td>
      <td>136.000000</td>
      <td>60.000000</td>
      <td>60.000000</td>
      <td>216.000000</td>
      <td>164.000000</td>
      <td>48.000000</td>
      <td>14.000000</td>
      <td>1.000000</td>
      <td>...</td>
      <td>144.000000</td>
      <td>685.000000</td>
      <td>603.000000</td>
      <td>130.000000</td>
      <td>143.000000</td>
      <td>22.000000</td>
      <td>13.000000</td>
      <td>27.000000</td>
      <td>184.000000</td>
      <td>60.000000</td>
    </tr>
  </tbody>
</table>
<p>8 rows × 26 columns</p>
</div>


### Excelファイルとして保存


```python
import pandas as pd
import os

# データの読み込みとチーム名の変換
description = '昨年翌年OPS本塁打'
file_path = f'./プロ野球データ打撃成績/batting_stats_{description}selected_2012_2023.xlsx'
data = pd.read_excel(file_path)

# 保存先ディレクトリの作成（存在しない場合）
output_dir = './プロ野球データ打撃成績_output/'
os.makedirs(output_dir, exist_ok=True)

# 基本的な情報をDataFrameに変換
info_df = pd.DataFrame({
    "Column": data.columns,
    "Non-Null Count": [data[col].notnull().sum() for col in data.columns],
    "Dtype": [data[col].dtype for col in data.columns]
})

# 統計量の取得
describe_df = data.describe()

# Excelファイルとして保存
output_file = os.path.join(output_dir, f'パリーグデータ_summary_{description}selected_2012_2023.xlsx')
with pd.ExcelWriter(output_file) as writer:
    info_df.to_excel(writer, sheet_name='基本情報', index=False)
    describe_df.to_excel(writer, sheet_name='統計量')

print(f"ファイルが {output_file} に保存されました。")

```

    ファイルが ./プロ野球データ打撃成績_output/パリーグデータ_summary_昨年翌年OPS本塁打_2012_2023.xlsx に保存されました。
    

### pairplot

#### data['翌年OPS']が0のレコードは削除


```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from scipy.stats import pearsonr
import matplotlib.font_manager as fm
import matplotlib as mpl
from matplotlib import rc

# 一時的に日本語フォントを設定
rc('font', family='Hiragino Sans')
# # 日本語フォントの設定
# jp_font_path = 'C:/Windows/Fonts/meiryo.ttc'  # 環境に合わせたフォントパス
# jp_font = fm.FontProperties(fname=jp_font_path)

# # グローバル設定で日本語フォントを適用
# mpl.rc('font', family=jp_font.get_name())

# データの読み込み
description = '翌年OPS本塁打'
file_path = f'./プロ野球データ打撃成績/batting_stats_{description}selected_2012_2023.xlsx'
data = pd.read_excel(file_path)

# Yearが2024のデータを除外
data = data[data['年度'] != 2023]

# 翌年OPSが0のレコードを削除
data = data[data['翌年OPS'] != 0]
# 打席数が0のレコードを削除
data = data[data['打席数'] != 0]

# # 除外する特徴量
# excluded_columns = [
#     '最近5試合', 'UC打数', 'UC安打', 'UC率', 'UC本塁打', '試合数'
# ]

# 使用する特徴量を選択し、'勝敗表_勝率'を含める
features = data.loc[:, '打率':'三振']
# features = data.drop(columns=excluded_columns).loc[:, '打率':'三振']
features['翌年本塁打'] = data['翌年本塁打']

# 保存先ディレクトリの作成
output_dir = './プロ野球データ打撃成績_output/'
os.makedirs(output_dir, exist_ok=True)

# 相関係数をプロットに表示する関数
def corrplot(x, y, **kws):
    r, _ = pearsonr(x, y)  # ピアソンの相関係数を計算
    ax = plt.gca()
    ax.annotate(f'ρ = {r:.2f}', xy=(0.5, 0.5), xycoords=ax.transAxes, 
                ha='center', va='center', fontsize=12, color='red')

# ペアプロットの作成（白黒印刷対応：マーカーを使い分け）
pairplot = sns.pairplot(
    features,
    markers=["o", "s", "D", "X", "^"],  # 白黒印刷用に異なるマーカーを設定
    plot_kws={'edgecolor': 'black', 'linewidth': 0.5}  # 白黒印刷用にエッジを黒で強調
)
# グラフ上に相関係数を追加
pairplot.map_lower(corrplot)

# コンソールに表示
plt.show()

# SVG形式で保存
output_file = os.path.join(output_dir, f'pairplot_{description}.svg')
pairplot.savefig(output_file, format='svg')

print(f"ペアプロットが {output_file} にSVG形式で保存されました。")

# JPG形式で保存
output_file_jpg = os.path.join(output_dir, f'pairplot_{description}.jpg')
pairplot.savefig(output_file_jpg, format='jpg', dpi=300)  # DPIを指定すると高解像度で保存可能

print(f"ペアプロットが {output_file_jpg} にJPG形式で保存されました。")

```

    /Applications/anaconda/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
      with pd.option_context('mode.use_inf_as_na', True):
    /Applications/anaconda/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
      with pd.option_context('mode.use_inf_as_na', True):
    /Applications/anaconda/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
      with pd.option_context('mode.use_inf_as_na', True):
    /Applications/anaconda/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
      with pd.option_context('mode.use_inf_as_na', True):
    /Applications/anaconda/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
      with pd.option_context('mode.use_inf_as_na', True):
    /Applications/anaconda/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
      with pd.option_context('mode.use_inf_as_na', True):
    /Applications/anaconda/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
      with pd.option_context('mode.use_inf_as_na', True):
    /Applications/anaconda/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
      with pd.option_context('mode.use_inf_as_na', True):
    /Applications/anaconda/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
      with pd.option_context('mode.use_inf_as_na', True):
    /Applications/anaconda/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
      with pd.option_context('mode.use_inf_as_na', True):
    /Applications/anaconda/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
      with pd.option_context('mode.use_inf_as_na', True):
    /Applications/anaconda/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
      with pd.option_context('mode.use_inf_as_na', True):
    /Applications/anaconda/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
      with pd.option_context('mode.use_inf_as_na', True):
    /Applications/anaconda/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
      with pd.option_context('mode.use_inf_as_na', True):
    /Applications/anaconda/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
      with pd.option_context('mode.use_inf_as_na', True):
    /Applications/anaconda/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
      with pd.option_context('mode.use_inf_as_na', True):
    /Applications/anaconda/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
      with pd.option_context('mode.use_inf_as_na', True):
    /Applications/anaconda/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
      with pd.option_context('mode.use_inf_as_na', True):
    /Applications/anaconda/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
      with pd.option_context('mode.use_inf_as_na', True):
    /Applications/anaconda/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
      with pd.option_context('mode.use_inf_as_na', True):
    /Applications/anaconda/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
      with pd.option_context('mode.use_inf_as_na', True):
    /Applications/anaconda/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
      with pd.option_context('mode.use_inf_as_na', True):
    /Applications/anaconda/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
      with pd.option_context('mode.use_inf_as_na', True):
    /Applications/anaconda/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
      with pd.option_context('mode.use_inf_as_na', True):
    /Applications/anaconda/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
      with pd.option_context('mode.use_inf_as_na', True):
    /Applications/anaconda/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
      with pd.option_context('mode.use_inf_as_na', True):
    /Applications/anaconda/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
      with pd.option_context('mode.use_inf_as_na', True):
    /Applications/anaconda/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
      with pd.option_context('mode.use_inf_as_na', True):
    /Applications/anaconda/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
      with pd.option_context('mode.use_inf_as_na', True):
    /Applications/anaconda/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
      with pd.option_context('mode.use_inf_as_na', True):
    /Applications/anaconda/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
      with pd.option_context('mode.use_inf_as_na', True):
    /Applications/anaconda/anaconda3/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.
      with pd.option_context('mode.use_inf_as_na', True):
    


    
![png](output_12_1.png)
    


    ペアプロットが ./プロ野球データ打撃成績_output/pairplot_翌年OPS本塁打.svg にSVG形式で保存されました。
    ペアプロットが ./プロ野球データ打撃成績_output/pairplot_翌年OPS本塁打.jpg にJPG形式で保存されました。
    

# 3. 回帰木 翌年度本塁打予測モデル 2012-2022(2023)

## 回帰木CART max_depth 最適値探索 学習用・検証用2y・テスト用2y


```python
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.tree import DecisionTreeRegressor
# from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.font_manager as fm
from scipy.stats import pearsonr  # 相関係数を計算するためにインポート

# 日本語フォントの設定
from matplotlib import rc
# 一時的に日本語フォントを設定
rc('font', family='Hiragino Sans')
# jp_font_path = 'C:/Windows/Fonts/meiryo.ttc'  # 環境に合わせたフォントパス
# jp_font = fm.FontProperties(fname=jp_font_path)

# 正規化スイッチの設定（Trueにすると正規化を適用）
normalize_flag = False

# 散布図の定義域設定（x軸とy軸の範囲）
xlim = (0, 10)  # x軸の範囲
ylim = (0, 10)  # y軸の範囲

# データの読み込み
description = '昨年翌年OPS本塁打'
file_path = f'./プロ野球データ打撃成績/batting_stats_{description}selected_2012_2023.xlsx'
data = pd.read_excel(file_path)

# Yearが2024のデータを除外
data = data[data['年度'] != 2023]

# # 「得失点差」特徴量を追加
# data['得失点差'] = data['打撃成績_得点'] - data['投手成績_失点']

# # 除外する特徴量
# excluded_columns = [
#     '最近5試合', 'UC打数', 'UC安打', 'UC率', 'UC本塁打', '試合数'
# ]

# 特徴量とターゲットの設定
# X = data.drop(columns=excluded_columns)
X = data.loc[:, '打率':'三振']

# 2012年-2018年のデータのみを使用して学習用データを設定
X_train = X[(data['年度'] >= 2012) & (data['年度'] <= 2018)]
y_train = data['翌年本塁打'][(data['年度'] >= 2012) & (data['年度'] <= 2018)]
# 2019年20年のデータのみを使用して検証用データを設定
X_valid = X[(data['年度'] == 2019) |(data['年度'] == 2020) ]
y_valid = data['翌年本塁打'][(data['年度'] == 2019) |(data['年度'] == 2020)]

X_test = X[(data['年度'] == 2021) |(data['年度'] == 2022)]
y_test = data['翌年本塁打'][ (data['年度'] == 2021) |(data['年度'] == 2022)]

teams_test = data['選手名'][(data['年度'] == 2021) |(data['年度'] == 2022)]
years_test = data['年度'][(data['年度'] == 2021) |(data['年度'] == 2022) ]


# 保存先ディレクトリの作成
output_folder = './プロ野球データ打撃成績_output/'
os.makedirs(output_folder, exist_ok=True)

# 学習用データ，検証用データとテスト用データをExcelに保存
with pd.ExcelWriter(os.path.join(output_folder, f'train_test_data_{description}.xlsx')) as writer:
    X_train.to_excel(writer, sheet_name='X_train', index=False)
    y_train.to_frame(name='翌年本塁打').to_excel(writer, sheet_name='y_train', index=False)
    X_valid.to_excel(writer, sheet_name='X_valid', index=False)
    y_valid.to_frame(name='翌年本塁打').to_excel(writer, sheet_name='y_valid', index=False)
    X_test.to_excel(writer, sheet_name='X_test', index=False)
    y_test.to_frame(name='翌年本塁打').to_excel(writer, sheet_name='y_test', index=False)

print("Training and test data saved to Excel file in output folder.")

# max_depthを変えながらR2スコアを計算する
max_depths = range(1, 21)  # max_depthを1から20まで試す
r2_scores = []

for depth in max_depths:
    model = DecisionTreeRegressor(random_state=42, max_depth=depth)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_valid)
    r2_scores.append(r2_score(y_valid, y_pred))

# max_depthとR2スコアのグラフを描画
plt.figure(figsize=(10, 6))
plt.plot(max_depths, r2_scores, marker='o')
plt.title('max_depthと決定係数R²の関係')
plt.xlabel('max_depth')
plt.ylabel('R²')
plt.xticks(ticks=max_depths)

# plt.title('max_depthと決定係数R²の関係', fontproperties=jp_font)
# plt.xlabel('max_depth', fontproperties=jp_font)
# plt.ylabel('R²', fontproperties=jp_font)
# plt.xticks(ticks=max_depths, fontproperties=jp_font)
# plt.yticks(fontproperties=jp_font)
plt.grid(True)
plt.savefig(os.path.join(output_folder, f'max_depth_vs_r2_{description}.svg'), bbox_inches='tight')
plt.show()
```

    Training and test data saved to Excel file in output folder.
    


    
![png](output_15_1.png)
    


## 回帰木CART  学習用・検証用2y・テスト用2y


```python
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.tree import DecisionTreeRegressor
# from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.font_manager as fm
from scipy.stats import pearsonr  # 相関係数を計算するためにインポート
# 日本語フォントの設定
from matplotlib import rc
# 一時的に日本語フォントを設定
rc('font', family='Hiragino Sans')
# # 日本語フォントの設定
# jp_font_path = 'C:/Windows/Fonts/meiryo.ttc'  # 環境に合わせたフォントパス
# jp_font = fm.FontProperties(fname=jp_font_path)

# 正規化スイッチの設定（Trueにすると正規化を適用）
normalize_flag = False

# 散布図の定義域設定（x軸とy軸の範囲）
xlim = (0, 60)  # x軸の範囲
ylim = (0, 60)  # y軸の範囲

# データの読み込み
description = '昨年翌年OPS本塁打'
file_path = f'./プロ野球データ打撃成績/batting_stats_{description}selected_2012_2023.xlsx'
data = pd.read_excel(file_path)

# チーム名を変換
# data['チーム名'] = data['チーム名'].replace(team_name_mapping)

# 「得失点差」特徴量を追加
# data['得失点差'] = data['打撃成績_得点'] - data['投手成績_失点']

# excluded_columns = [
#     '最近5試合', 'UC打数', 'UC安打', 'UC率', 'UC本塁打', '試合数'
# ]

# # 特徴量とターゲットの設定
# X = data.drop(columns=excluded_columns)
X = data.loc[:, '打率':'三振']
# X = X.loc[:, '打撃成績_打率':'得失点差']

# 2005年以降のデータのみを使用して学習用データを設定
X_train = X[(data['年度'] >= 2012) & (data['年度'] <= 2018)]
y_train = data['翌年本塁打'][(data['年度'] >= 2012) & (data['年度'] <= 2018)]
# 2005年以降のデータのみを使用して検証用データを設定
X_valid = X[(data['年度'] == 2019) |(data['年度'] == 2020) ]
y_valid = data['翌年本塁打'][(data['年度'] == 2019) |(data['年度'] == 2020)]

X_test = X[(data['年度'] == 2021) |(data['年度'] == 2022)]
y_test = data['翌年本塁打'][ (data['年度'] == 2021) |(data['年度'] == 2022)]

names_test = data['選手名'][(data['年度'] == 2021) |(data['年度'] == 2022)]
years_test = data['年度'][(data['年度'] == 2021) |(data['年度'] == 2022) ]


# 保存先ディレクトリの作成
output_folder = './プロ野球データ打撃成績_output/'
os.makedirs(output_folder, exist_ok=True)

# 学習用データ，検証用データとテスト用データをExcelに保存
with pd.ExcelWriter(os.path.join(output_folder, 'train_test_data.xlsx')) as writer:
    X_train.to_excel(writer, sheet_name='X_train', index=False)
    y_train.to_frame(name='翌年本塁打').to_excel(writer, sheet_name='y_train', index=False)
    X_valid.to_excel(writer, sheet_name='X_valid', index=False)
    y_valid.to_frame(name='翌年本塁打').to_excel(writer, sheet_name='y_valid', index=False)
    X_test.to_excel(writer, sheet_name='X_test', index=False)
    y_test.to_frame(name='翌年本塁打').to_excel(writer, sheet_name='y_test', index=False)

print("Training and test data saved to Excel file in output folder.")


# 決定木モデルの構築
model = DecisionTreeRegressor(random_state=42, max_depth=4)
model.fit(X_train, y_train)
y_pred = model.predict(X_valid)
y_train_pred = model.predict(X_train)  # 学習用データの予測値

# 特徴量重要度の可視化
plt.figure(figsize=(10, 6))
feature_importances = model.feature_importances_
sns.barplot(x=feature_importances, y=X.columns)
plt.title('特徴量の重要度')
plt.xlabel('重要度')
plt.ylabel('特徴量')
# plt.title('特徴量の重要度', fontproperties=jp_font)
# plt.xlabel('重要度', fontproperties=jp_font)
# plt.ylabel('特徴量', fontproperties=jp_font)
# plt.xticks(fontproperties=jp_font)
# plt.yticks(fontproperties=jp_font)
plt.savefig(os.path.join(output_folder, 'feature_importances.svg'), bbox_inches='tight')
plt.show()

# テストデータの予測値と実測値の相関を散布図で描画（チーム名と年を表示）
plt.figure(figsize=(12, 12))
plt.scatter(y_pred, y_valid, alpha=0.7)
plt.plot([0, 60], [0, 60], 'r--')
plt.xlim(xlim)
plt.ylim(ylim)

plt.title('テストデータ: 予測値 vs 実測値', fontsize=15)
plt.xlabel('予測値', fontsize=16)
plt.ylabel('実測値', fontsize=16)
plt.xticks(fontsize=15)
plt.yticks(fontsize=15)
# plt.title('テストデータ: 予測値 vs 実測値', fontproperties=jp_font, fontsize=15)
# plt.xlabel('予測値', fontproperties=jp_font, fontsize=16)
# plt.ylabel('実測値', fontproperties=jp_font, fontsize=16)
# plt.xticks(fontproperties=jp_font, fontsize=15)
# plt.yticks(fontproperties=jp_font, fontsize=15)
plt.savefig(os.path.join(output_folder, f'test_actual_vs_predicted_{description}.svg'), bbox_inches='tight')

# 学習データの予測値と実測値の相関を散布図で描画（チーム名と年は表示しない）
plt.figure(figsize=(12, 12))
plt.scatter(y_train_pred, y_train, alpha=0.7)
plt.plot([0, 60], [0, 60], 'r--')
plt.xlim(xlim)
plt.ylim(ylim)
plt.title('学習データ: 予測値 vs 実測値', fontsize=15)
plt.xlabel('予測値', fontsize=16)
plt.ylabel('実測値', fontsize=16)
plt.xticks(fontsize=15)
plt.yticks(fontsize=15)
# plt.title('学習データ: 予測値 vs 実測値', fontproperties=jp_font, fontsize=15)
# plt.xlabel('予測値', fontproperties=jp_font, fontsize=16)
# plt.ylabel('実測値', fontproperties=jp_font, fontsize=16)
# plt.xticks(fontproperties=jp_font, fontsize=15)
# plt.yticks(fontproperties=jp_font, fontsize=15)
plt.savefig(os.path.join(output_folder, f'train_actual_vs_predicted_{description}.svg'), bbox_inches='tight')

# モデル評価
mse_test = mean_squared_error(y_valid, y_pred)
r2_test = r2_score(y_valid, y_pred)
correlation_test, _ = pearsonr(y_valid, y_pred)

mse_train = mean_squared_error(y_train, y_train_pred)
r2_train = r2_score(y_train, y_train_pred)
correlation_train, _ = pearsonr(y_train, y_train_pred)

# 評価結果を保存
evaluation_results = (
    f"Test Data:\n"
    f"Mean Squared Error (MSE): {mse_test}\nR-squared (R²): {r2_test}\nCorrelation Coefficient (r): {correlation_test}\n\n"
    f"Train Data:\n"
    f"Mean Squared Error (MSE): {mse_train}\nR-squared (R²): {r2_train}\nCorrelation Coefficient (r): {correlation_train}"
)
with open(os.path.join(output_folder, f'model_evaluation_{description}.txt'), 'w', encoding='utf-8-sig') as f:
    f.write(evaluation_results)

print(f"Graphs and evaluation results saved to {output_folder}")
print(evaluation_results)
plt.show()

```

    Training and test data saved to Excel file in output folder.
    


    
![png](output_17_1.png)
    


    Graphs and evaluation results saved to ./プロ野球データ打撃成績_output/
    Test Data:
    Mean Squared Error (MSE): 18.046909489827556
    R-squared (R²): 0.4109369502482263
    Correlation Coefficient (r): 0.6812733969132188
    
    Train Data:
    Mean Squared Error (MSE): 11.81429704110726
    R-squared (R²): 0.6607639457005747
    Correlation Coefficient (r): 0.8128738805624045
    


    
![png](output_17_3.png)
    



    
![png](output_17_4.png)
    


# 4. GBM 翌年度本塁打予測モデル 2012-2022(2023)

## GBM max_depth 最適値探索 学習用・検証用2y・テスト用2y


```python
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.tree import DecisionTreeRegressor
# from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.font_manager as fm
from scipy.stats import pearsonr  # 相関係数を計算するためにインポート

# 日本語フォントの設定
jp_font_path = 'C:/Windows/Fonts/meiryo.ttc'  # 環境に合わせたフォントパス
jp_font = fm.FontProperties(fname=jp_font_path)

# 正規化スイッチの設定（Trueにすると正規化を適用）
normalize_flag = False

# 散布図の定義域設定（x軸とy軸の範囲）
xlim = (0, 10)  # x軸の範囲
ylim = (0, 10)  # y軸の範囲

# データの読み込み
description = '翌年OPS本塁打'
file_path = f'./プロ野球データ打撃成績/batting_stats_{description}_2012_2023.xlsx'
data = pd.read_excel(file_path)

# Yearが2024のデータを除外
data = data[data['年度'] != 2023]

# # 「得失点差」特徴量を追加
# data['得失点差'] = data['打撃成績_得点'] - data['投手成績_失点']

# 除外する特徴量
excluded_columns = [
    '最近5試合', 'UC打数', 'UC安打', 'UC率', 'UC本塁打', '試合数'
]

'''【UC打率】 勝敗更新機会時打率、VDUCP(Victory or defeat update chance point)の略
データで楽しむプロ野球(baseballdata.jp)の管理人が考案。 
この打席でホームランを打てば同点、先制、勝ち越し、逆転となる打席における打率。
つまり、ここ一番打率。'''


# 特徴量とターゲットの設定
X = data.drop(columns=excluded_columns)
X = X.loc[:, '打率':'三振']

# 2012年-2018年のデータのみを使用して学習用データを設定
X_train = X[(data['年度'] >= 2012) & (data['年度'] <= 2018)]
y_train = data['翌年本塁打'][(data['年度'] >= 2012) & (data['年度'] <= 2018)]
# 2019年20年のデータのみを使用して検証用データを設定
X_valid = X[(data['年度'] == 2019) |(data['年度'] == 2020) ]
y_valid = data['翌年本塁打'][(data['年度'] == 2019) |(data['年度'] == 2020)]

X_test = X[(data['年度'] == 2021) |(data['年度'] == 2022)]
y_test = data['翌年本塁打'][ (data['年度'] == 2021) |(data['年度'] == 2022)]

teams_test = data['選手名'][(data['年度'] == 2021) |(data['年度'] == 2022)]
years_test = data['年度'][(data['年度'] == 2021) |(data['年度'] == 2022) ]


# 保存先ディレクトリの作成
output_folder = './プロ野球データ打撃成績_output/'
os.makedirs(output_folder, exist_ok=True)

# 学習用データ，検証用データとテスト用データをExcelに保存
with pd.ExcelWriter(os.path.join(output_folder, f'train_test_data_{description}.xlsx')) as writer:
    X_train.to_excel(writer, sheet_name='X_train', index=False)
    y_train.to_frame(name='翌年本塁打').to_excel(writer, sheet_name='y_train', index=False)
    X_valid.to_excel(writer, sheet_name='X_valid', index=False)
    y_valid.to_frame(name='翌年本塁打').to_excel(writer, sheet_name='y_valid', index=False)
    X_test.to_excel(writer, sheet_name='X_test', index=False)
    y_test.to_frame(name='翌年本塁打').to_excel(writer, sheet_name='y_test', index=False)

print("Training and test data saved to Excel file in output folder.")

# max_depthを変えながらR2スコアを計算する
max_depths = range(1, 21)  # max_depthを1から20まで試す
r2_scores = []

for depth in max_depths:
    model = GradientBoostingRegressor(random_state=42, n_estimators=100, max_depth=depth)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_valid)
    r2_scores.append(r2_score(y_valid, y_pred))

# max_depthとR2スコアのグラフを描画
plt.figure(figsize=(10, 6))
plt.plot(max_depths, r2_scores, marker='o')
plt.title('max_depthと決定係数R²の関係', fontproperties=jp_font)
plt.xlabel('max_depth', fontproperties=jp_font)
plt.ylabel('R²', fontproperties=jp_font)
plt.xticks(ticks=max_depths, fontproperties=jp_font)
plt.yticks(fontproperties=jp_font)
plt.grid(True)
plt.savefig(os.path.join(output_folder, f'max_depth_vs_r2_{description}.svg'), bbox_inches='tight')
plt.show()

# import os
# import pandas as pd
# import numpy as np
# import matplotlib.pyplot as plt
# import seaborn as sns
# from sklearn.ensemble import GradientBoostingRegressor
# from sklearn.metrics import mean_squared_error, r2_score
# import matplotlib.font_manager as fm
# from scipy.stats import pearsonr  # 相関係数を計算するためにインポート

# # 日本語フォントの設定
# jp_font_path = 'C:/Windows/Fonts/meiryo.ttc'  # 環境に合わせたフォントパス
# jp_font = fm.FontProperties(fname=jp_font_path)

# # 正規化スイッチの設定（Trueにすると正規化を適用）
# normalize_flag = False

# # 散布図の定義域設定（x軸とy軸の範囲）
# xlim = (0.3, 0.7)  # x軸の範囲
# ylim = (0.3, 0.7)  # y軸の範囲

# # チーム名の変換マッピング
# team_name_mapping = {
#     '千葉ロッテマリーンズ': 'M',
#     '福岡ソフトバンクホークス': 'H',
#     '西武ライオンズ': 'L',
#     '埼玉西武ライオンズ': 'L',
#     'オリックス・バファローズ': 'B',
#     '北海道日本ハムファイターズ': 'F',
#     '東北楽天ゴールデンイーグルス': 'E'
# }

# # ファイルのロード
# file_path = './プロ野球データ/パリーグデータ_2005_2024.xlsx'
# data = pd.read_excel(file_path)

# # チーム名を変換
# data['チーム名'] = data['チーム名'].replace(team_name_mapping)

# # 「得失点差」特徴量を追加
# data['得失点差'] = data['打撃成績_得点'] - data['投手成績_失点']

# # 除外する特徴量を定義
# excluded_columns = [
#     '打撃成績_試合', '打撃成績_打数', '投手成績_勝利', '投手成績_敗北', 
#     '投手成績_投球回', '投手成績_試合', '投手成績_', '勝敗表_ゲーム差'
# ]

# # 増分を取る対象の特徴量のみを選択
# # features = data.loc[:, '打撃成績_打率':'得失点差'].drop(columns=excluded_columns)

# # 特徴量とターゲットの設定
# X = data.drop(columns=excluded_columns)
# X = X.loc[:, '打撃成績_打率':'得失点差']  # 元の特徴量
# # X = pd.concat([X, data[increment_columns]], axis=1)  # 増分特徴量と「得失点差」を追加

# # 2005年以降のデータのみを使用して学習用データを設定
# X_train = X[(data['Year'] >= 2005) & (data['Year'] <= 2021)]
# y_train = data['勝敗表_勝率'][(data['Year'] >= 2005) & (data['Year'] <= 2021)]
# # 2005年以降のデータのみを使用して検証用データを設定
# X_valid = X[(data['Year'] == 2022) | (data['Year'] == 2023)]
# y_valid = data['勝敗表_勝率'][(data['Year'] == 2022) | (data['Year'] == 2023)]

# X_test = X[(data['Year'] == 2024)]
# y_test = data['勝敗表_勝率'][ (data['Year'] == 2024)]

# teams_test = data['チーム名'][(data['Year'] == 2024)]
# years_test = data['Year'][(data['Year'] == 2024)]


# # outputフォルダーの作成
# output_folder = './プロ野球パリーグ_output_CART3y_当該年勝率_2005_2024/'
# if not os.path.exists(output_folder):
#     os.makedirs(output_folder)

# # 学習用データ，検証用データとテスト用データをExcelに保存
# with pd.ExcelWriter(os.path.join(output_folder, 'train_test_data.xlsx')) as writer:
#     X_train.to_excel(writer, sheet_name='X_train', index=False)
#     y_train.to_frame(name='勝敗表_勝率').to_excel(writer, sheet_name='y_train', index=False)
#     X_valid.to_excel(writer, sheet_name='X_valid', index=False)
#     y_valid.to_frame(name='勝敗表_勝率').to_excel(writer, sheet_name='y_valid', index=False)
#     X_test.to_excel(writer, sheet_name='X_test', index=False)
#     y_test.to_frame(name='勝敗表_勝率').to_excel(writer, sheet_name='y_test', index=False)

# print("Training and test data saved to Excel file in output folder.")

# # max_depthを変えながらR2スコアを計算する
# max_depths = range(1, 21)  # max_depthを1から20まで試す
# r2_scores = []

# for depth in max_depths:
#     model = GradientBoostingRegressor(random_state=42, n_estimators=100, max_depth=depth)
#     model.fit(X_train, y_train)
#     y_pred = model.predict(X_valid)
#     r2_scores.append(r2_score(y_valid, y_pred))

# # max_depthとR2スコアのグラフを描画
# plt.figure(figsize=(10, 6))
# plt.plot(max_depths, r2_scores, marker='o')
# plt.title('max_depthと決定係数R²の関係', fontproperties=jp_font)
# plt.xlabel('max_depth', fontproperties=jp_font)
# plt.ylabel('R²', fontproperties=jp_font)
# plt.xticks(ticks=max_depths, fontproperties=jp_font)
# plt.yticks(fontproperties=jp_font)
# plt.grid(True)
# plt.savefig(os.path.join(output_folder, 'max_depth_vs_r2.svg'), bbox_inches='tight')
# plt.show()
```

    Training and test data saved to Excel file in output folder.
    


    
![png](output_20_1.png)
    


## GBM回帰モデル構築


```python
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import GradientBoostingRegressor
# from sklearn.tree import DecisionTreeRegressor
# from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.font_manager as fm
from scipy.stats import pearsonr  # 相関係数を計算するためにインポート
# 日本語フォントの設定
from matplotlib import rc
# 一時的に日本語フォントを設定
rc('font', family='Hiragino Sans')
# # 日本語フォントの設定
# jp_font_path = 'C:/Windows/Fonts/meiryo.ttc'  # 環境に合わせたフォントパス
# jp_font = fm.FontProperties(fname=jp_font_path)

# 正規化スイッチの設定（Trueにすると正規化を適用）
normalize_flag = False

# 散布図の定義域設定（x軸とy軸の範囲）
xlim = (0, 60)  # x軸の範囲
ylim = (0, 60)  # y軸の範囲

# データの読み込み
description = '昨年翌年OPS本塁打'
file_path = f'./プロ野球データ打撃成績/batting_stats_{description}selected_2012_2023.xlsx'
data = pd.read_excel(file_path)

# チーム名を変換
# data['チーム名'] = data['チーム名'].replace(team_name_mapping)

# 「得失点差」特徴量を追加
# data['得失点差'] = data['打撃成績_得点'] - data['投手成績_失点']

# excluded_columns = [
#     '最近5試合', 'UC打数', 'UC安打', 'UC率', 'UC本塁打', '試合数'
# ]

# # 特徴量とターゲットの設定
# X = data.drop(columns=excluded_columns)
X = data.loc[:, '打率':'三振']
# X = X.loc[:, '打撃成績_打率':'得失点差']

# 2005年以降のデータのみを使用して学習用データを設定
X_train = X[(data['年度'] >= 2012) & (data['年度'] <= 2018)]
y_train = data['翌年本塁打'][(data['年度'] >= 2012) & (data['年度'] <= 2018)]
# 2005年以降のデータのみを使用して検証用データを設定
X_valid = X[(data['年度'] == 2019) |(data['年度'] == 2020) ]
y_valid = data['翌年本塁打'][(data['年度'] == 2019) |(data['年度'] == 2020)]

X_test = X[(data['年度'] == 2021) |(data['年度'] == 2022)]
y_test = data['翌年本塁打'][ (data['年度'] == 2021) |(data['年度'] == 2022)]

names_test = data['選手名'][(data['年度'] == 2021) |(data['年度'] == 2022)]
years_test = data['年度'][(data['年度'] == 2021) |(data['年度'] == 2022) ]


# 保存先ディレクトリの作成
output_folder = './プロ野球データ打撃成績_output/'
os.makedirs(output_folder, exist_ok=True)

# 学習用データ，検証用データとテスト用データをExcelに保存
with pd.ExcelWriter(os.path.join(output_folder, 'train_test_data.xlsx')) as writer:
    X_train.to_excel(writer, sheet_name='X_train', index=False)
    y_train.to_frame(name='翌年本塁打').to_excel(writer, sheet_name='y_train', index=False)
    X_valid.to_excel(writer, sheet_name='X_valid', index=False)
    y_valid.to_frame(name='翌年本塁打').to_excel(writer, sheet_name='y_valid', index=False)
    X_test.to_excel(writer, sheet_name='X_test', index=False)
    y_test.to_frame(name='翌年本塁打').to_excel(writer, sheet_name='y_test', index=False)

print("Training and test data saved to Excel file in output folder.")


# 決定木モデルの構築
model = GradientBoostingRegressor(random_state=42, n_estimators=100, max_depth=4)
# model = DecisionTreeRegressor(random_state=42, max_depth=4)
model.fit(X_train, y_train)
y_pred = model.predict(X_valid)
y_train_pred = model.predict(X_train)  # 学習用データの予測値

# 特徴量重要度の可視化
plt.figure(figsize=(10, 6))
feature_importances = model.feature_importances_
sns.barplot(x=feature_importances, y=X.columns)
plt.title('特徴量の重要度')
plt.xlabel('重要度')
plt.ylabel('特徴量')
# plt.title('特徴量の重要度', fontproperties=jp_font)
# plt.xlabel('重要度', fontproperties=jp_font)
# plt.ylabel('特徴量', fontproperties=jp_font)
# plt.xticks(fontproperties=jp_font)
# plt.yticks(fontproperties=jp_font)
plt.savefig(os.path.join(output_folder, 'feature_importances.svg'), bbox_inches='tight')
plt.show()

# テストデータの予測値と実測値の相関を散布図で描画（チーム名と年を表示）
plt.figure(figsize=(12, 12))
plt.scatter(y_pred, y_valid, alpha=0.7)
plt.plot([0, 60], [0, 60], 'r--')
plt.xlim(xlim)
plt.ylim(ylim)

plt.title('テストデータ: 予測値 vs 実測値', fontsize=15)
plt.xlabel('予測値', fontsize=16)
plt.ylabel('実測値', fontsize=16)
plt.xticks(fontsize=15)
plt.yticks(fontsize=15)
# plt.title('テストデータ: 予測値 vs 実測値', fontproperties=jp_font, fontsize=15)
# plt.xlabel('予測値', fontproperties=jp_font, fontsize=16)
# plt.ylabel('実測値', fontproperties=jp_font, fontsize=16)
# plt.xticks(fontproperties=jp_font, fontsize=15)
# plt.yticks(fontproperties=jp_font, fontsize=15)
plt.savefig(os.path.join(output_folder, f'test_actual_vs_predicted_{description}.svg'), bbox_inches='tight')

# 学習データの予測値と実測値の相関を散布図で描画（チーム名と年は表示しない）
plt.figure(figsize=(12, 12))
plt.scatter(y_train_pred, y_train, alpha=0.7)
plt.plot([0, 60], [0, 60], 'r--')
plt.xlim(xlim)
plt.ylim(ylim)
plt.title('学習データ: 予測値 vs 実測値', fontsize=15)
plt.xlabel('予測値', fontsize=16)
plt.ylabel('実測値', fontsize=16)
plt.xticks(fontsize=15)
plt.yticks(fontsize=15)
# plt.title('学習データ: 予測値 vs 実測値', fontproperties=jp_font, fontsize=15)
# plt.xlabel('予測値', fontproperties=jp_font, fontsize=16)
# plt.ylabel('実測値', fontproperties=jp_font, fontsize=16)
# plt.xticks(fontproperties=jp_font, fontsize=15)
# plt.yticks(fontproperties=jp_font, fontsize=15)
plt.savefig(os.path.join(output_folder, f'train_actual_vs_predicted_{description}.svg'), bbox_inches='tight')

# モデル評価
mse_test = mean_squared_error(y_valid, y_pred)
r2_test = r2_score(y_valid, y_pred)
correlation_test, _ = pearsonr(y_valid, y_pred)

mse_train = mean_squared_error(y_train, y_train_pred)
r2_train = r2_score(y_train, y_train_pred)
correlation_train, _ = pearsonr(y_train, y_train_pred)

# 評価結果を保存
evaluation_results = (
    f"Test Data:\n"
    f"Mean Squared Error (MSE): {mse_test}\nR-squared (R²): {r2_test}\nCorrelation Coefficient (r): {correlation_test}\n\n"
    f"Train Data:\n"
    f"Mean Squared Error (MSE): {mse_train}\nR-squared (R²): {r2_train}\nCorrelation Coefficient (r): {correlation_train}"
)
with open(os.path.join(output_folder, f'model_evaluation_{description}.txt'), 'w', encoding='utf-8-sig') as f:
    f.write(evaluation_results)

print(f"Graphs and evaluation results saved to {output_folder}")
print(evaluation_results)
plt.show()
```

    Training and test data saved to Excel file in output folder.
    


    
![png](output_22_1.png)
    


    Graphs and evaluation results saved to ./プロ野球データ打撃成績_output/
    Test Data:
    Mean Squared Error (MSE): 18.638787427298844
    R-squared (R²): 0.3916176633019405
    Correlation Coefficient (r): 0.7083711637380556
    
    Train Data:
    Mean Squared Error (MSE): 4.464297326509195
    R-squared (R²): 0.871812042223503
    Correlation Coefficient (r): 0.9362365235144309
    


    
![png](output_22_3.png)
    



    
![png](output_22_4.png)
    


# 5. ランダムフォレスト 翌年度本塁打予測モデル 2012-2022(2023)

## ランダムフォレスト max_depth 最適値探索 学習用・検証用2y・テスト用2y


```python
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.font_manager as fm
from scipy.stats import pearsonr  # 相関係数を計算するためにインポート

# 日本語フォントの設定
jp_font_path = 'C:/Windows/Fonts/meiryo.ttc'  # 環境に合わせたフォントパス
jp_font = fm.FontProperties(fname=jp_font_path)

# 正規化スイッチの設定（Trueにすると正規化を適用）
normalize_flag = False

# 散布図の定義域設定（x軸とy軸の範囲）
xlim = (0, 10)  # x軸の範囲
ylim = (0, 10)  # y軸の範囲

# データの読み込み
description = '翌年OPS本塁打'
file_path = f'./プロ野球データ打撃成績/batting_stats_{description}_2012_2023.xlsx'
data = pd.read_excel(file_path)

# Yearが2024のデータを除外
data = data[data['年度'] != 2023]

# # 「得失点差」特徴量を追加
# data['得失点差'] = data['打撃成績_得点'] - data['投手成績_失点']

# 除外する特徴量
excluded_columns = [
    '最近5試合', 'UC打数', 'UC安打', 'UC率', 'UC本塁打', '試合数'
]

'''【UC打率】 勝敗更新機会時打率、VDUCP(Victory or defeat update chance point)の略
データで楽しむプロ野球(baseballdata.jp)の管理人が考案。 
この打席でホームランを打てば同点、先制、勝ち越し、逆転となる打席における打率。
つまり、ここ一番打率。'''


# 特徴量とターゲットの設定
X = data.drop(columns=excluded_columns)
X = X.loc[:, '打率':'三振']

# 2012年-2018年のデータのみを使用して学習用データを設定
X_train = X[(data['年度'] >= 2012) & (data['年度'] <= 2018)]
y_train = data['翌年本塁打'][(data['年度'] >= 2012) & (data['年度'] <= 2018)]
# 2019年20年のデータのみを使用して検証用データを設定
X_valid = X[(data['年度'] == 2019) |(data['年度'] == 2020) ]
y_valid = data['翌年本塁打'][(data['年度'] == 2019) |(data['年度'] == 2020)]

X_test = X[(data['年度'] == 2021) |(data['年度'] == 2022)]
y_test = data['翌年本塁打'][ (data['年度'] == 2021) |(data['年度'] == 2022)]

teams_test = data['選手名'][(data['年度'] == 2021) |(data['年度'] == 2022)]
years_test = data['年度'][(data['年度'] == 2021) |(data['年度'] == 2022) ]


# 保存先ディレクトリの作成
output_folder = './プロ野球データ打撃成績_output/'
os.makedirs(output_folder, exist_ok=True)

# 学習用データ，検証用データとテスト用データをExcelに保存
with pd.ExcelWriter(os.path.join(output_folder, f'train_test_data_{description}.xlsx')) as writer:
    X_train.to_excel(writer, sheet_name='X_train', index=False)
    y_train.to_frame(name='翌年本塁打').to_excel(writer, sheet_name='y_train', index=False)
    X_valid.to_excel(writer, sheet_name='X_valid', index=False)
    y_valid.to_frame(name='翌年本塁打').to_excel(writer, sheet_name='y_valid', index=False)
    X_test.to_excel(writer, sheet_name='X_test', index=False)
    y_test.to_frame(name='翌年本塁打').to_excel(writer, sheet_name='y_test', index=False)

print("Training and test data saved to Excel file in output folder.")

# max_depthを変えながらR2スコアを計算する
max_depths = range(1, 21)  # max_depthを1から20まで試す
r2_scores = []

for depth in max_depths:
    model = RandomForestRegressor(random_state=42, n_estimators=100, max_depth=depth)
    # model = DecisionTreeRegressor(random_state=42, max_depth=depth)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_valid)
    r2_scores.append(r2_score(y_valid, y_pred))

# max_depthとR2スコアのグラフを描画
plt.figure(figsize=(10, 6))
plt.plot(max_depths, r2_scores, marker='o')
plt.title('max_depthと決定係数R²の関係', fontproperties=jp_font)
plt.xlabel('max_depth', fontproperties=jp_font)
plt.ylabel('R²', fontproperties=jp_font)
plt.xticks(ticks=max_depths, fontproperties=jp_font)
plt.yticks(fontproperties=jp_font)
plt.grid(True)
plt.savefig(os.path.join(output_folder, f'max_depth_vs_r2_{description}.svg'), bbox_inches='tight')
plt.show()


```

    Training and test data saved to Excel file in output folder.
    


    
![png](output_25_1.png)
    


## ランダムフォレスト 回帰モデル構築


```python
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# from sklearn.ensemble import GradientBoostingRegressor
# from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.font_manager as fm
from scipy.stats import pearsonr  # 相関係数を計算するためにインポート
# 日本語フォントの設定
from matplotlib import rc
# 一時的に日本語フォントを設定
rc('font', family='Hiragino Sans')
# # 日本語フォントの設定
# jp_font_path = 'C:/Windows/Fonts/meiryo.ttc'  # 環境に合わせたフォントパス
# jp_font = fm.FontProperties(fname=jp_font_path)

# 正規化スイッチの設定（Trueにすると正規化を適用）
normalize_flag = False

# 散布図の定義域設定（x軸とy軸の範囲）
xlim = (0, 60)  # x軸の範囲
ylim = (0, 60)  # y軸の範囲

# データの読み込み
description = '昨年翌年OPS本塁打'
file_path = f'./プロ野球データ打撃成績/batting_stats_{description}selected_2012_2023.xlsx'
data = pd.read_excel(file_path)

# チーム名を変換
# data['チーム名'] = data['チーム名'].replace(team_name_mapping)

# 「得失点差」特徴量を追加
# data['得失点差'] = data['打撃成績_得点'] - data['投手成績_失点']

# excluded_columns = [
#     '最近5試合', 'UC打数', 'UC安打', 'UC率', 'UC本塁打', '試合数'
# ]

# # 特徴量とターゲットの設定
# X = data.drop(columns=excluded_columns)
X = data.loc[:, '打率':'三振']
# X = X.loc[:, '打撃成績_打率':'得失点差']

# 2005年以降のデータのみを使用して学習用データを設定
X_train = X[(data['年度'] >= 2012) & (data['年度'] <= 2018)]
y_train = data['翌年本塁打'][(data['年度'] >= 2012) & (data['年度'] <= 2018)]
# 2005年以降のデータのみを使用して検証用データを設定
X_valid = X[(data['年度'] == 2019) |(data['年度'] == 2020) ]
y_valid = data['翌年本塁打'][(data['年度'] == 2019) |(data['年度'] == 2020)]

X_test = X[(data['年度'] == 2021) |(data['年度'] == 2022)]
y_test = data['翌年本塁打'][ (data['年度'] == 2021) |(data['年度'] == 2022)]

names_test = data['選手名'][(data['年度'] == 2021) |(data['年度'] == 2022)]
years_test = data['年度'][(data['年度'] == 2021) |(data['年度'] == 2022) ]


# 保存先ディレクトリの作成
output_folder = './プロ野球データ打撃成績_output/'
os.makedirs(output_folder, exist_ok=True)

# 学習用データ，検証用データとテスト用データをExcelに保存
with pd.ExcelWriter(os.path.join(output_folder, 'train_test_data.xlsx')) as writer:
    X_train.to_excel(writer, sheet_name='X_train', index=False)
    y_train.to_frame(name='翌年本塁打').to_excel(writer, sheet_name='y_train', index=False)
    X_valid.to_excel(writer, sheet_name='X_valid', index=False)
    y_valid.to_frame(name='翌年本塁打').to_excel(writer, sheet_name='y_valid', index=False)
    X_test.to_excel(writer, sheet_name='X_test', index=False)
    y_test.to_frame(name='翌年本塁打').to_excel(writer, sheet_name='y_test', index=False)

print("Training and test data saved to Excel file in output folder.")


# 決定木モデルの構築
model = RandomForestRegressor(random_state=42, n_estimators=100, max_depth=6)
# model = GradientBoostingRegressor(random_state=42, n_estimators=100, max_depth=4)
# model = DecisionTreeRegressor(random_state=42, max_depth=4)
model.fit(X_train, y_train)
y_pred = model.predict(X_valid)
y_train_pred = model.predict(X_train)  # 学習用データの予測値

# 特徴量重要度の可視化
plt.figure(figsize=(10, 6))
feature_importances = model.feature_importances_
sns.barplot(x=feature_importances, y=X.columns)
plt.title('特徴量の重要度')
plt.xlabel('重要度')
plt.ylabel('特徴量')
# plt.title('特徴量の重要度', fontproperties=jp_font)
# plt.xlabel('重要度', fontproperties=jp_font)
# plt.ylabel('特徴量', fontproperties=jp_font)
# plt.xticks(fontproperties=jp_font)
# plt.yticks(fontproperties=jp_font)
plt.savefig(os.path.join(output_folder, 'feature_importances.svg'), bbox_inches='tight')
plt.show()

# テストデータの予測値と実測値の相関を散布図で描画（チーム名と年を表示）
plt.figure(figsize=(12, 12))
plt.scatter(y_pred, y_valid, alpha=0.7)
plt.plot([0, 60], [0, 60], 'r--')
plt.xlim(xlim)
plt.ylim(ylim)

plt.title('テストデータ: 予測値 vs 実測値', fontsize=15)
plt.xlabel('予測値', fontsize=16)
plt.ylabel('実測値', fontsize=16)
plt.xticks(fontsize=15)
plt.yticks(fontsize=15)
# plt.title('テストデータ: 予測値 vs 実測値', fontproperties=jp_font, fontsize=15)
# plt.xlabel('予測値', fontproperties=jp_font, fontsize=16)
# plt.ylabel('実測値', fontproperties=jp_font, fontsize=16)
# plt.xticks(fontproperties=jp_font, fontsize=15)
# plt.yticks(fontproperties=jp_font, fontsize=15)
plt.savefig(os.path.join(output_folder, f'test_actual_vs_predicted_{description}.svg'), bbox_inches='tight')

# 学習データの予測値と実測値の相関を散布図で描画（チーム名と年は表示しない）
plt.figure(figsize=(12, 12))
plt.scatter(y_train_pred, y_train, alpha=0.7)
plt.plot([0, 60], [0, 60], 'r--')
plt.xlim(xlim)
plt.ylim(ylim)
plt.title('学習データ: 予測値 vs 実測値', fontsize=15)
plt.xlabel('予測値', fontsize=16)
plt.ylabel('実測値', fontsize=16)
plt.xticks(fontsize=15)
plt.yticks(fontsize=15)
# plt.title('学習データ: 予測値 vs 実測値', fontproperties=jp_font, fontsize=15)
# plt.xlabel('予測値', fontproperties=jp_font, fontsize=16)
# plt.ylabel('実測値', fontproperties=jp_font, fontsize=16)
# plt.xticks(fontproperties=jp_font, fontsize=15)
# plt.yticks(fontproperties=jp_font, fontsize=15)
plt.savefig(os.path.join(output_folder, f'train_actual_vs_predicted_{description}.svg'), bbox_inches='tight')

# モデル評価
mse_test = mean_squared_error(y_valid, y_pred)
r2_test = r2_score(y_valid, y_pred)
correlation_test, _ = pearsonr(y_valid, y_pred)

mse_train = mean_squared_error(y_train, y_train_pred)
r2_train = r2_score(y_train, y_train_pred)
correlation_train, _ = pearsonr(y_train, y_train_pred)

# 評価結果を保存
evaluation_results = (
    f"Test Data:\n"
    f"Mean Squared Error (MSE): {mse_test}\nR-squared (R²): {r2_test}\nCorrelation Coefficient (r): {correlation_test}\n\n"
    f"Train Data:\n"
    f"Mean Squared Error (MSE): {mse_train}\nR-squared (R²): {r2_train}\nCorrelation Coefficient (r): {correlation_train}"
)
with open(os.path.join(output_folder, f'model_evaluation_{description}.txt'), 'w', encoding='utf-8-sig') as f:
    f.write(evaluation_results)

print(f"Graphs and evaluation results saved to {output_folder}")
print(evaluation_results)
plt.show()
# import os
# import pandas as pd
# import numpy as np
# import matplotlib.pyplot as plt
# import seaborn as sns
# # from sklearn.ensemble import GradientBoostingRegressor
# from sklearn.ensemble import RandomForestRegressor

# from sklearn.metrics import mean_squared_error, r2_score
# import matplotlib.font_manager as fm
# from scipy.stats import pearsonr  # 相関係数を計算するためにインポート

# # 日本語フォントの設定
# jp_font_path = 'C:/Windows/Fonts/meiryo.ttc'  # 環境に合わせたフォントパス
# jp_font = fm.FontProperties(fname=jp_font_path)

# # 正規化スイッチの設定（Trueにすると正規化を適用）
# normalize_flag = False

# # 散布図の定義域設定（x軸とy軸の範囲）
# xlim = (0, 60)  # x軸の範囲
# ylim = (0, 60)  # y軸の範囲

# # データの読み込み
# description = '翌年OPS本塁打'
# file_path = f'./プロ野球データ打撃成績/batting_stats_{description}_2012_2023.xlsx'
# data = pd.read_excel(file_path)

# # チーム名を変換
# # data['チーム名'] = data['チーム名'].replace(team_name_mapping)

# # 「得失点差」特徴量を追加
# # data['得失点差'] = data['打撃成績_得点'] - data['投手成績_失点']

# excluded_columns = [
#     '最近5試合', 'UC打数', 'UC安打', 'UC率', 'UC本塁打', '試合数'
# ]

# '''【UC打率】 勝敗更新機会時打率、VDUCP(Victory or defeat update chance point)の略
# データで楽しむプロ野球(baseballdata.jp)の管理人が考案。 
# この打席でホームランを打てば同点、先制、勝ち越し、逆転となる打席における打率。
# つまり、ここ一番打率。'''


# # 特徴量とターゲットの設定
# X = data.drop(columns=excluded_columns)
# X = X.loc[:, '打率':'三振']
# # X = X.loc[:, '打撃成績_打率':'得失点差']

# # 2005年以降のデータのみを使用して学習用データを設定
# X_train = X[(data['年度'] >= 2012) & (data['年度'] <= 2018)]
# y_train = data['翌年本塁打'][(data['年度'] >= 2012) & (data['年度'] <= 2018)]
# # 2005年以降のデータのみを使用して検証用データを設定
# X_valid = X[(data['年度'] == 2019) |(data['年度'] == 2020) ]
# y_valid = data['翌年本塁打'][(data['年度'] == 2019) |(data['年度'] == 2020)]

# X_test = X[(data['年度'] == 2021) |(data['年度'] == 2022)]
# y_test = data['翌年本塁打'][ (data['年度'] == 2021) |(data['年度'] == 2022)]

# names_test = data['選手名'][(data['年度'] == 2021) |(data['年度'] == 2022)]
# years_test = data['年度'][(data['年度'] == 2021) |(data['年度'] == 2022) ]


# # 保存先ディレクトリの作成
# output_folder = './プロ野球データ打撃成績_output/'
# os.makedirs(output_folder, exist_ok=True)

# # 学習用データ，検証用データとテスト用データをExcelに保存
# with pd.ExcelWriter(os.path.join(output_folder, 'train_test_data.xlsx')) as writer:
#     X_train.to_excel(writer, sheet_name='X_train', index=False)
#     y_train.to_frame(name='翌年本塁打').to_excel(writer, sheet_name='y_train', index=False)
#     X_valid.to_excel(writer, sheet_name='X_valid', index=False)
#     y_valid.to_frame(name='翌年本塁打').to_excel(writer, sheet_name='y_valid', index=False)
#     X_test.to_excel(writer, sheet_name='X_test', index=False)
#     y_test.to_frame(name='翌年本塁打').to_excel(writer, sheet_name='y_test', index=False)

# print("Training and test data saved to Excel file in output folder.")


# # 決定木モデルの構築
# model = RandomForestRegressor(random_state=42, n_estimators=100, max_depth=6)
# model.fit(X_train, y_train)
# y_pred = model.predict(X_valid)
# y_train_pred = model.predict(X_train)  # 学習用データの予測値

# # 特徴量重要度の可視化
# plt.figure(figsize=(10, 6))
# feature_importances = model.feature_importances_
# sns.barplot(x=feature_importances, y=X.columns)
# plt.title('特徴量の重要度', fontproperties=jp_font)
# plt.xlabel('重要度', fontproperties=jp_font)
# plt.ylabel('特徴量', fontproperties=jp_font)
# plt.xticks(fontproperties=jp_font)
# plt.yticks(fontproperties=jp_font)
# plt.savefig(os.path.join(output_folder, 'feature_importances.svg'), bbox_inches='tight')
# plt.show()

# # テストデータの予測値と実測値の相関を散布図で描画（チーム名と年を表示）
# plt.figure(figsize=(12, 12))
# plt.scatter(y_pred, y_valid, alpha=0.7)
# plt.plot([0, 60], [0, 60], 'r--')
# plt.xlim(xlim)
# plt.ylim(ylim)

# plt.title('テストデータ: 予測値 vs 実測値', fontproperties=jp_font, fontsize=15)
# plt.xlabel('予測値', fontproperties=jp_font, fontsize=16)
# plt.ylabel('実測値', fontproperties=jp_font, fontsize=16)
# plt.xticks(fontproperties=jp_font, fontsize=15)
# plt.yticks(fontproperties=jp_font, fontsize=15)
# plt.savefig(os.path.join(output_folder, f'test_actual_vs_predicted_{description}.svg'), bbox_inches='tight')

# # 学習データの予測値と実測値の相関を散布図で描画（チーム名と年は表示しない）
# plt.figure(figsize=(12, 12))
# plt.scatter(y_train_pred, y_train, alpha=0.7)
# plt.plot([0, 60], [0, 60], 'r--')
# plt.xlim(xlim)
# plt.ylim(ylim)
# plt.title('学習データ: 予測値 vs 実測値', fontproperties=jp_font, fontsize=15)
# plt.xlabel('予測値', fontproperties=jp_font, fontsize=16)
# plt.ylabel('実測値', fontproperties=jp_font, fontsize=16)
# plt.xticks(fontproperties=jp_font, fontsize=15)
# plt.yticks(fontproperties=jp_font, fontsize=15)
# plt.savefig(os.path.join(output_folder, f'train_actual_vs_predicted_{description}.svg'), bbox_inches='tight')

# # モデル評価
# mse_test = mean_squared_error(y_valid, y_pred)
# r2_test = r2_score(y_valid, y_pred)
# correlation_test, _ = pearsonr(y_valid, y_pred)

# mse_train = mean_squared_error(y_train, y_train_pred)
# r2_train = r2_score(y_train, y_train_pred)
# correlation_train, _ = pearsonr(y_train, y_train_pred)

# # 評価結果を保存
# evaluation_results = (
#     f"Test Data:\n"
#     f"Mean Squared Error (MSE): {mse_test}\nR-squared (R²): {r2_test}\nCorrelation Coefficient (r): {correlation_test}\n\n"
#     f"Train Data:\n"
#     f"Mean Squared Error (MSE): {mse_train}\nR-squared (R²): {r2_train}\nCorrelation Coefficient (r): {correlation_train}"
# )
# with open(os.path.join(output_folder, f'model_evaluation_{description}.txt'), 'w', encoding='utf-8-sig') as f:
#     f.write(evaluation_results)

# print(f"Graphs and evaluation results saved to {output_folder}")
# print(evaluation_results)
# plt.show()

```

    Training and test data saved to Excel file in output folder.
    


    
![png](output_27_1.png)
    


    Graphs and evaluation results saved to ./プロ野球データ打撃成績_output/
    Test Data:
    Mean Squared Error (MSE): 17.231297173245537
    R-squared (R²): 0.4375590751550823
    Correlation Coefficient (r): 0.7092293414065756
    
    Train Data:
    Mean Squared Error (MSE): 7.254617855918629
    R-squared (R²): 0.7916907008238436
    Correlation Coefficient (r): 0.893728727406091
    


    
![png](output_27_3.png)
    



    
![png](output_27_4.png)
    


# 6. LightGBM 翌年度本塁打予測モデル 2012-2022(2023)

## LightGBM  max_depth 最適値探索 学習用・検証用2y・テスト用2y


```python
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from lightgbm import LGBMRegressor
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.font_manager as fm
from scipy.stats import pearsonr  # 相関係数を計算するためにインポート

# 日本語フォントの設定
jp_font_path = 'C:/Windows/Fonts/meiryo.ttc'  # 環境に合わせたフォントパス
jp_font = fm.FontProperties(fname=jp_font_path)

# 正規化スイッチの設定（Trueにすると正規化を適用）
normalize_flag = False

# 散布図の定義域設定（x軸とy軸の範囲）
xlim = (0, 10)  # x軸の範囲
ylim = (0, 10)  # y軸の範囲

# データの読み込み
description = '翌年OPS本塁打'
file_path = f'./プロ野球データ打撃成績/batting_stats_{description}_2012_2023.xlsx'
data = pd.read_excel(file_path)

# Yearが2024のデータを除外
data = data[data['年度'] != 2023]

# # 「得失点差」特徴量を追加
# data['得失点差'] = data['打撃成績_得点'] - data['投手成績_失点']

# 除外する特徴量
excluded_columns = [
    '最近5試合', 'UC打数', 'UC安打', 'UC率', 'UC本塁打', '試合数'
]

'''【UC打率】 勝敗更新機会時打率、VDUCP(Victory or defeat update chance point)の略
データで楽しむプロ野球(baseballdata.jp)の管理人が考案。 
この打席でホームランを打てば同点、先制、勝ち越し、逆転となる打席における打率。
つまり、ここ一番打率。'''


# 特徴量とターゲットの設定
X = data.drop(columns=excluded_columns)
X = X.loc[:, '打率':'三振']

# 2012年-2018年のデータのみを使用して学習用データを設定
X_train = X[(data['年度'] >= 2012) & (data['年度'] <= 2018)]
y_train = data['翌年本塁打'][(data['年度'] >= 2012) & (data['年度'] <= 2018)]
# 2019年20年のデータのみを使用して検証用データを設定
X_valid = X[(data['年度'] == 2019) |(data['年度'] == 2020) ]
y_valid = data['翌年本塁打'][(data['年度'] == 2019) |(data['年度'] == 2020)]

X_test = X[(data['年度'] == 2021) |(data['年度'] == 2022)]
y_test = data['翌年本塁打'][ (data['年度'] == 2021) |(data['年度'] == 2022)]

teams_test = data['選手名'][(data['年度'] == 2021) |(data['年度'] == 2022)]
years_test = data['年度'][(data['年度'] == 2021) |(data['年度'] == 2022) ]


# 保存先ディレクトリの作成
output_folder = './プロ野球データ打撃成績_output/'
os.makedirs(output_folder, exist_ok=True)

# 学習用データ，検証用データとテスト用データをExcelに保存
with pd.ExcelWriter(os.path.join(output_folder, f'train_test_data_{description}.xlsx')) as writer:
    X_train.to_excel(writer, sheet_name='X_train', index=False)
    y_train.to_frame(name='翌年本塁打').to_excel(writer, sheet_name='y_train', index=False)
    X_valid.to_excel(writer, sheet_name='X_valid', index=False)
    y_valid.to_frame(name='翌年本塁打').to_excel(writer, sheet_name='y_valid', index=False)
    X_test.to_excel(writer, sheet_name='X_test', index=False)
    y_test.to_frame(name='翌年本塁打').to_excel(writer, sheet_name='y_test', index=False)

print("Training and test data saved to Excel file in output folder.")

# max_depthを変えながらR2スコアを計算する
max_depths = range(1, 21)  # max_depthを1から20まで試す
r2_scores = []

for depth in max_depths:
    model = LGBMRegressor(random_state=42, n_estimators=100, max_depth=depth)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_valid)
    r2_scores.append(r2_score(y_valid, y_pred))

# max_depthとR2スコアのグラフを描画
plt.figure(figsize=(10, 6))
plt.plot(max_depths, r2_scores, marker='o')
plt.title('max_depthと決定係数R²の関係', fontproperties=jp_font)
plt.xlabel('max_depth', fontproperties=jp_font)
plt.ylabel('R²', fontproperties=jp_font)
plt.xticks(ticks=max_depths, fontproperties=jp_font)
plt.yticks(fontproperties=jp_font)
plt.grid(True)
plt.savefig(os.path.join(output_folder, f'max_depth_vs_r2_{description}.svg'), bbox_inches='tight')
plt.show()



# import os
# import pandas as pd
# import numpy as np
# import matplotlib.pyplot as plt
# import seaborn as sns
# from lightgbm import LGBMRegressor
# from sklearn.metrics import mean_squared_error, r2_score
# import matplotlib.font_manager as fm
# from scipy.stats import pearsonr  # 相関係数を計算するためにインポート

# # 日本語フォントの設定
# jp_font_path = 'C:/Windows/Fonts/meiryo.ttc'  # 環境に合わせたフォントパス
# jp_font = fm.FontProperties(fname=jp_font_path)

# # 正規化スイッチの設定（Trueにすると正規化を適用）
# normalize_flag = False

# # 散布図の定義域設定（x軸とy軸の範囲）
# xlim = (0.3, 0.7)  # x軸の範囲
# ylim = (0.3, 0.7)  # y軸の範囲

# # チーム名の変換マッピング
# team_name_mapping = {
#     '千葉ロッテマリーンズ': 'M',
#     '福岡ソフトバンクホークス': 'H',
#     '西武ライオンズ': 'L',
#     '埼玉西武ライオンズ': 'L',
#     'オリックス・バファローズ': 'B',
#     '北海道日本ハムファイターズ': 'F',
#     '東北楽天ゴールデンイーグルス': 'E'
# }

# # ファイルのロード
# file_path = './プロ野球データ/パリーグデータ_2005_2024.xlsx'
# data = pd.read_excel(file_path)

# # チーム名を変換
# data['チーム名'] = data['チーム名'].replace(team_name_mapping)

# # 「得失点差」特徴量を追加
# data['得失点差'] = data['打撃成績_得点'] - data['投手成績_失点']

# # 除外する特徴量を定義
# excluded_columns = [
#     '打撃成績_試合', '打撃成績_打数', '投手成績_勝利', '投手成績_敗北', 
#     '投手成績_投球回', '投手成績_試合', '投手成績_', '勝敗表_ゲーム差'
# ]

# # 特徴量とターゲットの設定
# X = data.drop(columns=excluded_columns)
# X = X.loc[:, '打撃成績_打率':'得失点差']  # 元の特徴量
# # X = pd.concat([X, data[increment_columns]], axis=1)  # 増分特徴量と「得失点差」を追加

# # 2006年以降のデータのみを使用して学習データを設定
# X_train = X[(data['Year'] >= 2005) & (data['Year'] <= 2021)]
# y_train = data['勝敗表_勝率'][(data['Year'] >= 2005) & (data['Year'] <= 2021)]

# X_test = X[(data['Year'] == 2022) | (data['Year'] == 2023) | (data['Year'] == 2024)]
# y_test = data['勝敗表_勝率'][(data['Year'] == 2022) | (data['Year'] == 2023) | (data['Year'] == 2024)]

# teams_test = data['チーム名'][(data['Year'] == 2022) | (data['Year'] == 2023) | (data['Year'] == 2024)]
# years_test = data['Year'][(data['Year'] == 2022) | (data['Year'] == 2023) | (data['Year'] == 2024)]


# # outputフォルダーの作成
# output_folder = './プロ野球パリーグ_output_LGBM3y_当該年勝率_2005_2024/'
# if not os.path.exists(output_folder):
#     os.makedirs(output_folder)

# # 学習用データとテスト用データをExcelに保存
# with pd.ExcelWriter(os.path.join(output_folder, 'train_test_data.xlsx')) as writer:
#     X_train.to_excel(writer, sheet_name='X_train', index=False)
#     y_train.to_frame(name='勝敗表_勝率').to_excel(writer, sheet_name='y_train', index=False)
#     X_test.to_excel(writer, sheet_name='X_test', index=False)
#     y_test.to_frame(name='勝敗表_勝率').to_excel(writer, sheet_name='y_test', index=False)

# print("Training and test data saved to Excel file in output folder.")

# # max_depthを変えながらR2スコアを計算する
# max_depths = range(1, 21)  # max_depthを1から20まで試す
# r2_scores = []

# for depth in max_depths:
#     model = LGBMRegressor(random_state=42, n_estimators=100, max_depth=depth)
#     model.fit(X_train, y_train)
#     y_pred = model.predict(X_test)
#     r2_scores.append(r2_score(y_test, y_pred))

# # max_depthとR2スコアのグラフを描画
# plt.figure(figsize=(10, 6))
# plt.plot(max_depths, r2_scores, marker='o')
# plt.title('max_depthと決定係数R²の関係', fontproperties=jp_font)
# plt.xlabel('max_depth', fontproperties=jp_font)
# plt.ylabel('R²', fontproperties=jp_font)
# plt.xticks(ticks=max_depths, fontproperties=jp_font)
# plt.yticks(fontproperties=jp_font)
# plt.grid(True)
# plt.savefig(os.path.join(output_folder, 'max_depth_vs_r2.svg'), bbox_inches='tight')
# plt.show()
```

    Training and test data saved to Excel file in output folder.
    [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001863 seconds.
    You can set `force_row_wise=true` to remove the overhead.
    And if memory is not enough, you can set `force_col_wise=true`.
    [LightGBM] [Info] Total Bins 3286
    [LightGBM] [Info] Number of data points in the train set: 3975, number of used features: 31
    [LightGBM] [Info] Start training from score 2.240503
    [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000564 seconds.
    You can set `force_col_wise=true` to remove the overhead.
    [LightGBM] [Info] Total Bins 3286
    [LightGBM] [Info] Number of data points in the train set: 3975, number of used features: 31
    [LightGBM] [Info] Start training from score 2.240503
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000593 seconds.
    You can set `force_col_wise=true` to remove the overhead.
    [LightGBM] [Info] Total Bins 3286
    [LightGBM] [Info] Number of data points in the train set: 3975, number of used features: 31
    [LightGBM] [Info] Start training from score 2.240503
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000543 seconds.
    You can set `force_col_wise=true` to remove the overhead.
    [LightGBM] [Info] Total Bins 3286
    [LightGBM] [Info] Number of data points in the train set: 3975, number of used features: 31
    [LightGBM] [Info] Start training from score 2.240503
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000503 seconds.
    You can set `force_col_wise=true` to remove the overhead.
    [LightGBM] [Info] Total Bins 3286
    [LightGBM] [Info] Number of data points in the train set: 3975, number of used features: 31
    [LightGBM] [Info] Start training from score 2.240503
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000484 seconds.
    You can set `force_col_wise=true` to remove the overhead.
    [LightGBM] [Info] Total Bins 3286
    [LightGBM] [Info] Number of data points in the train set: 3975, number of used features: 31
    [LightGBM] [Info] Start training from score 2.240503
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000492 seconds.
    You can set `force_col_wise=true` to remove the overhead.
    [LightGBM] [Info] Total Bins 3286
    [LightGBM] [Info] Number of data points in the train set: 3975, number of used features: 31
    [LightGBM] [Info] Start training from score 2.240503
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000612 seconds.
    You can set `force_col_wise=true` to remove the overhead.
    [LightGBM] [Info] Total Bins 3286
    [LightGBM] [Info] Number of data points in the train set: 3975, number of used features: 31
    [LightGBM] [Info] Start training from score 2.240503
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000486 seconds.
    You can set `force_col_wise=true` to remove the overhead.
    [LightGBM] [Info] Total Bins 3286
    [LightGBM] [Info] Number of data points in the train set: 3975, number of used features: 31
    [LightGBM] [Info] Start training from score 2.240503
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000517 seconds.
    You can set `force_col_wise=true` to remove the overhead.
    [LightGBM] [Info] Total Bins 3286
    [LightGBM] [Info] Number of data points in the train set: 3975, number of used features: 31
    [LightGBM] [Info] Start training from score 2.240503
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000625 seconds.
    You can set `force_col_wise=true` to remove the overhead.
    [LightGBM] [Info] Total Bins 3286
    [LightGBM] [Info] Number of data points in the train set: 3975, number of used features: 31
    [LightGBM] [Info] Start training from score 2.240503
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000614 seconds.
    You can set `force_col_wise=true` to remove the overhead.
    [LightGBM] [Info] Total Bins 3286
    [LightGBM] [Info] Number of data points in the train set: 3975, number of used features: 31
    [LightGBM] [Info] Start training from score 2.240503
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000541 seconds.
    You can set `force_col_wise=true` to remove the overhead.
    [LightGBM] [Info] Total Bins 3286
    [LightGBM] [Info] Number of data points in the train set: 3975, number of used features: 31
    [LightGBM] [Info] Start training from score 2.240503
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000628 seconds.
    You can set `force_col_wise=true` to remove the overhead.
    [LightGBM] [Info] Total Bins 3286
    [LightGBM] [Info] Number of data points in the train set: 3975, number of used features: 31
    [LightGBM] [Info] Start training from score 2.240503
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000576 seconds.
    You can set `force_col_wise=true` to remove the overhead.
    [LightGBM] [Info] Total Bins 3286
    [LightGBM] [Info] Number of data points in the train set: 3975, number of used features: 31
    [LightGBM] [Info] Start training from score 2.240503
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000724 seconds.
    You can set `force_col_wise=true` to remove the overhead.
    [LightGBM] [Info] Total Bins 3286
    [LightGBM] [Info] Number of data points in the train set: 3975, number of used features: 31
    [LightGBM] [Info] Start training from score 2.240503
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000553 seconds.
    You can set `force_col_wise=true` to remove the overhead.
    [LightGBM] [Info] Total Bins 3286
    [LightGBM] [Info] Number of data points in the train set: 3975, number of used features: 31
    [LightGBM] [Info] Start training from score 2.240503
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000651 seconds.
    You can set `force_col_wise=true` to remove the overhead.
    [LightGBM] [Info] Total Bins 3286
    [LightGBM] [Info] Number of data points in the train set: 3975, number of used features: 31
    [LightGBM] [Info] Start training from score 2.240503
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000689 seconds.
    You can set `force_col_wise=true` to remove the overhead.
    [LightGBM] [Info] Total Bins 3286
    [LightGBM] [Info] Number of data points in the train set: 3975, number of used features: 31
    [LightGBM] [Info] Start training from score 2.240503
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000514 seconds.
    You can set `force_col_wise=true` to remove the overhead.
    [LightGBM] [Info] Total Bins 3286
    [LightGBM] [Info] Number of data points in the train set: 3975, number of used features: 31
    [LightGBM] [Info] Start training from score 2.240503
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).
    


    
![png](output_30_1.png)
    


## LightGBM 回帰モデル構築


```python
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from lightgbm import LGBMRegressor

from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.font_manager as fm
from scipy.stats import pearsonr  # 相関係数を計算するためにインポート

# 日本語フォントの設定
jp_font_path = 'C:/Windows/Fonts/meiryo.ttc'  # 環境に合わせたフォントパス
jp_font = fm.FontProperties(fname=jp_font_path)

# 正規化スイッチの設定（Trueにすると正規化を適用）
normalize_flag = False

# 散布図の定義域設定（x軸とy軸の範囲）
xlim = (0, 60)  # x軸の範囲
ylim = (0, 60)  # y軸の範囲

# データの読み込み
description = '翌年OPS本塁打'
file_path = f'./プロ野球データ打撃成績/batting_stats_{description}_2012_2023.xlsx'
data = pd.read_excel(file_path)

# チーム名を変換
# data['チーム名'] = data['チーム名'].replace(team_name_mapping)

# 「得失点差」特徴量を追加
# data['得失点差'] = data['打撃成績_得点'] - data['投手成績_失点']

excluded_columns = [
    '最近5試合', 'UC打数', 'UC安打', 'UC率', 'UC本塁打', '試合数'
]

'''【UC打率】 勝敗更新機会時打率、VDUCP(Victory or defeat update chance point)の略
データで楽しむプロ野球(baseballdata.jp)の管理人が考案。 
この打席でホームランを打てば同点、先制、勝ち越し、逆転となる打席における打率。
つまり、ここ一番打率。'''


# 特徴量とターゲットの設定
X = data.drop(columns=excluded_columns)
X = X.loc[:, '打率':'三振']
# X = X.loc[:, '打撃成績_打率':'得失点差']

# 2005年以降のデータのみを使用して学習用データを設定
X_train = X[(data['年度'] >= 2012) & (data['年度'] <= 2018)]
y_train = data['翌年本塁打'][(data['年度'] >= 2012) & (data['年度'] <= 2018)]
# 2005年以降のデータのみを使用して検証用データを設定
X_valid = X[(data['年度'] == 2019) |(data['年度'] == 2020) ]
y_valid = data['翌年本塁打'][(data['年度'] == 2019) |(data['年度'] == 2020)]

X_test = X[(data['年度'] == 2021) |(data['年度'] == 2022)]
y_test = data['翌年本塁打'][ (data['年度'] == 2021) |(data['年度'] == 2022)]

names_test = data['選手名'][(data['年度'] == 2021) |(data['年度'] == 2022)]
years_test = data['年度'][(data['年度'] == 2021) |(data['年度'] == 2022) ]


# 保存先ディレクトリの作成
output_folder = './プロ野球データ打撃成績_output/'
os.makedirs(output_folder, exist_ok=True)

# 学習用データ，検証用データとテスト用データをExcelに保存
with pd.ExcelWriter(os.path.join(output_folder, 'train_test_data.xlsx')) as writer:
    X_train.to_excel(writer, sheet_name='X_train', index=False)
    y_train.to_frame(name='翌年本塁打').to_excel(writer, sheet_name='y_train', index=False)
    X_valid.to_excel(writer, sheet_name='X_valid', index=False)
    y_valid.to_frame(name='翌年本塁打').to_excel(writer, sheet_name='y_valid', index=False)
    X_test.to_excel(writer, sheet_name='X_test', index=False)
    y_test.to_frame(name='翌年本塁打').to_excel(writer, sheet_name='y_test', index=False)

print("Training and test data saved to Excel file in output folder.")


# 決定木モデルの構築
model = LGBMRegressor(random_state=42, n_estimators=100, max_depth=4)
model.fit(X_train, y_train)
y_pred = model.predict(X_valid)
y_train_pred = model.predict(X_train)  # 学習用データの予測値

# 特徴量重要度の可視化
plt.figure(figsize=(10, 6))
feature_importances = model.feature_importances_
sns.barplot(x=feature_importances, y=X.columns)
plt.title('特徴量の重要度', fontproperties=jp_font)
plt.xlabel('重要度', fontproperties=jp_font)
plt.ylabel('特徴量', fontproperties=jp_font)
plt.xticks(fontproperties=jp_font)
plt.yticks(fontproperties=jp_font)
plt.savefig(os.path.join(output_folder, 'feature_importances.svg'), bbox_inches='tight')
plt.show()

# テストデータの予測値と実測値の相関を散布図で描画（チーム名と年を表示）
plt.figure(figsize=(12, 12))
plt.scatter(y_pred, y_valid, alpha=0.7)
plt.plot([0, 60], [0, 60], 'r--')
plt.xlim(xlim)
plt.ylim(ylim)

plt.title('テストデータ: 予測値 vs 実測値', fontproperties=jp_font, fontsize=15)
plt.xlabel('予測値', fontproperties=jp_font, fontsize=16)
plt.ylabel('実測値', fontproperties=jp_font, fontsize=16)
plt.xticks(fontproperties=jp_font, fontsize=15)
plt.yticks(fontproperties=jp_font, fontsize=15)
plt.savefig(os.path.join(output_folder, f'test_actual_vs_predicted_{description}.svg'), bbox_inches='tight')

# 学習データの予測値と実測値の相関を散布図で描画（チーム名と年は表示しない）
plt.figure(figsize=(12, 12))
plt.scatter(y_train_pred, y_train, alpha=0.7)
plt.plot([0, 60], [0, 60], 'r--')
plt.xlim(xlim)
plt.ylim(ylim)
plt.title('学習データ: 予測値 vs 実測値', fontproperties=jp_font, fontsize=15)
plt.xlabel('予測値', fontproperties=jp_font, fontsize=16)
plt.ylabel('実測値', fontproperties=jp_font, fontsize=16)
plt.xticks(fontproperties=jp_font, fontsize=15)
plt.yticks(fontproperties=jp_font, fontsize=15)
plt.savefig(os.path.join(output_folder, f'train_actual_vs_predicted_{description}.svg'), bbox_inches='tight')

# モデル評価
mse_test = mean_squared_error(y_valid, y_pred)
r2_test = r2_score(y_valid, y_pred)
correlation_test, _ = pearsonr(y_valid, y_pred)

mse_train = mean_squared_error(y_train, y_train_pred)
r2_train = r2_score(y_train, y_train_pred)
correlation_train, _ = pearsonr(y_train, y_train_pred)

# 評価結果を保存
evaluation_results = (
    f"Test Data:\n"
    f"Mean Squared Error (MSE): {mse_test}\nR-squared (R²): {r2_test}\nCorrelation Coefficient (r): {correlation_test}\n\n"
    f"Train Data:\n"
    f"Mean Squared Error (MSE): {mse_train}\nR-squared (R²): {r2_train}\nCorrelation Coefficient (r): {correlation_train}"
)
with open(os.path.join(output_folder, f'model_evaluation_{description}.txt'), 'w', encoding='utf-8-sig') as f:
    f.write(evaluation_results)

print(f"Graphs and evaluation results saved to {output_folder}")
print(evaluation_results)
plt.show()


# import os
# import pandas as pd
# import numpy as np
# import matplotlib.pyplot as plt
# import seaborn as sns
# from lightgbm import LGBMRegressor
# from sklearn.metrics import mean_squared_error, r2_score
# import matplotlib.font_manager as fm
# from scipy.stats import pearsonr  # 相関係数を計算するためにインポート

# # 日本語フォントの設定
# jp_font_path = 'C:/Windows/Fonts/meiryo.ttc'  # 環境に合わせたフォントパス
# jp_font = fm.FontProperties(fname=jp_font_path)

# # 正規化スイッチの設定（Trueにすると正規化を適用）
# normalize_flag = False

# # 散布図の定義域設定（x軸とy軸の範囲）
# xlim = (0.3, 0.7)  # x軸の範囲
# ylim = (0.3, 0.7)  # y軸の範囲

# # チーム名の変換マッピング
# team_name_mapping = {
#     '千葉ロッテマリーンズ': 'M',
#     '福岡ソフトバンクホークス': 'H',
#     '西武ライオンズ': 'L',
#     '埼玉西武ライオンズ': 'L',
#     'オリックス・バファローズ': 'B',
#     '北海道日本ハムファイターズ': 'F',
#     '東北楽天ゴールデンイーグルス': 'E'
# }

# # ファイルのロード
# file_path = './プロ野球データ/パリーグデータ_2005_2024.xlsx'
# data = pd.read_excel(file_path)

# # チーム名を変換
# data['チーム名'] = data['チーム名'].replace(team_name_mapping)

# # 「得失点差」特徴量を追加
# data['得失点差'] = data['打撃成績_得点'] - data['投手成績_失点']

# # 除外する特徴量を定義
# excluded_columns = [
#     '打撃成績_試合', '打撃成績_打数', '投手成績_勝利', '投手成績_敗北', 
#     '投手成績_投球回', '投手成績_試合', '投手成績_','勝敗表_ゲーム差'
# ]

# # 増分を取る対象の特徴量のみを選択
# features = data.loc[:, '打撃成績_打率':'得失点差'].drop(columns=excluded_columns)

# # 特徴量とターゲットの設定
# X = data.drop(columns=excluded_columns)
# X = X.loc[:, '打撃成績_打率':'得失点差']  # 元の特徴量
# # X = pd.concat([X, data[increment_columns]], axis=1)  # 増分特徴量と「得失点差」を追加

# # 2005年-2021年のデータを使用して学習データを設定
# X_train = X[(data['Year'] >= 2005) & (data['Year'] <= 2021)]
# y_train = data['勝敗表_勝率'][(data['Year'] >= 2005) & (data['Year'] <= 2021)]

# X_test = X[(data['Year'] == 2022) | (data['Year'] == 2023) | (data['Year'] == 2024)]
# y_test = data['勝敗表_勝率'][(data['Year'] == 2022) | (data['Year'] == 2023) | (data['Year'] == 2024)]

# teams_test = data['チーム名'][(data['Year'] == 2022) | (data['Year'] == 2023) | (data['Year'] == 2024)]
# years_test = data['Year'][(data['Year'] == 2022) | (data['Year'] == 2023) | (data['Year'] == 2024)]

# # outputフォルダーの作成
# output_folder = './プロ野球パリーグ_output_LGBM3y_当該年勝率_2005_2024/'
# if not os.path.exists(output_folder):
#     os.makedirs(output_folder)

# # 学習用データとテスト用データをExcelに保存
# with pd.ExcelWriter(os.path.join(output_folder, 'train_test_data.xlsx')) as writer:
#     X_train.to_excel(writer, sheet_name='X_train', index=False)
#     y_train.to_frame(name='勝敗表_勝率').to_excel(writer, sheet_name='y_train', index=False)
#     X_test.to_excel(writer, sheet_name='X_test', index=False)
#     y_test.to_frame(name='勝敗表_勝率').to_excel(writer, sheet_name='y_test', index=False)

# print("Training and test data saved to Excel file in output folder.")

# # LightGBMの構築
# model = LGBMRegressor(random_state=42, n_estimators=100, max_depth=4, num_leaves=16, force_col_wise=True)
# model.fit(X_train, y_train)
# y_pred = model.predict(X_test)
# y_train_pred = model.predict(X_train)  # 学習用データの予測値

# # 特徴量重要度の可視化
# plt.figure(figsize=(10, 6))
# feature_importances = model.feature_importances_
# sns.barplot(x=feature_importances, y=X.columns)
# plt.title('特徴量の重要度', fontproperties=jp_font)
# plt.xlabel('重要度', fontproperties=jp_font)
# plt.ylabel('特徴量', fontproperties=jp_font)
# plt.xticks(fontproperties=jp_font)
# plt.yticks(fontproperties=jp_font)
# plt.savefig(os.path.join(output_folder, 'feature_importances.svg'), bbox_inches='tight')
# plt.show()

# # テストデータの予測値と実測値の相関を散布図で描画（チーム名と年を表示）
# plt.figure(figsize=(12, 12))
# plt.scatter(y_pred, y_test, alpha=0.7)
# plt.plot([0, 1], [0, 1], 'r--')
# plt.xlim(xlim)
# plt.ylim(ylim)

# # チーム名とYearを斜めにプロット、重ならないようにオフセットを設定
# offset_increment = 0.004  # オフセット増分
# extra_offset = 0.003  # マーカーから文字を離す距離
# placed_positions = []  # すでに配置された位置を記録

# for i in range(len(y_test)):
#     y_position = y_test.iloc[i]
#     x_position = y_pred[i] + extra_offset
    
#     # 重なりチェック
#     for (existing_x, existing_y) in placed_positions:
#         if abs(y_position - existing_y) < offset_increment and abs(x_position - existing_x) < offset_increment:
#             y_position += offset_increment  # 新しく配置するテキストだけを動かす
#             break  # 最初に見つけた重なりのみ動かす

#     # 現在の位置を記録
#     placed_positions.append((x_position, y_position))
    
#     # テキストを表示
#     plt.text(
#         x_position, y_position, 
#         f"{teams_test.iloc[i]} ({years_test.iloc[i]})", 
#         fontproperties=jp_font, fontsize=10, rotation=20  # テキストを20度傾ける
#     )

# plt.title('テストデータ: 予測値 vs 実測値', fontproperties=jp_font, fontsize=15)
# plt.xlabel('予測値', fontproperties=jp_font, fontsize=16)
# plt.ylabel('実測値', fontproperties=jp_font, fontsize=16)
# plt.xticks(fontproperties=jp_font, fontsize=15)
# plt.yticks(fontproperties=jp_font, fontsize=15)
# plt.savefig(os.path.join(output_folder, 'test_actual_vs_predicted.svg'), bbox_inches='tight')

# # 学習データの予測値と実測値の相関を散布図で描画（チーム名と年は表示しない）
# plt.figure(figsize=(12, 12))
# plt.scatter(y_train_pred, y_train, alpha=0.7)
# plt.plot([0, 1], [0, 1], 'r--')
# plt.xlim(xlim)
# plt.ylim(ylim)
# plt.title('学習データ: 予測値 vs 実測値', fontproperties=jp_font, fontsize=15)
# plt.xlabel('予測値', fontproperties=jp_font, fontsize=16)
# plt.ylabel('実測値', fontproperties=jp_font, fontsize=16)
# plt.xticks(fontproperties=jp_font, fontsize=15)
# plt.yticks(fontproperties=jp_font, fontsize=15)
# plt.savefig(os.path.join(output_folder, 'train_actual_vs_predicted.svg'), bbox_inches='tight')

# # モデル評価
# mse_test = mean_squared_error(y_test, y_pred)
# r2_test = r2_score(y_test, y_pred)
# correlation_test, _ = pearsonr(y_test, y_pred)

# mse_train = mean_squared_error(y_train, y_train_pred)
# r2_train = r2_score(y_train, y_train_pred)
# correlation_train, _ = pearsonr(y_train, y_train_pred)

# # 評価結果を保存
# evaluation_results = (
#     f"Test Data:\n"
#     f"Mean Squared Error (MSE): {mse_test}\nR-squared (R²): {r2_test}\nCorrelation Coefficient (r): {correlation_test}\n\n"
#     f"Train Data:\n"
#     f"Mean Squared Error (MSE): {mse_train}\nR-squared (R²): {r2_train}\nCorrelation Coefficient (r): {correlation_train}"
# )
# with open(os.path.join(output_folder, 'model_evaluation.txt'), 'w', encoding='utf-8') as f:
#     f.write(evaluation_results)

# print(f"Graphs and evaluation results saved to {output_folder}")
# print(evaluation_results)
# plt.show()

```

    Training and test data saved to Excel file in output folder.
    [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000668 seconds.
    You can set `force_col_wise=true` to remove the overhead.
    [LightGBM] [Info] Total Bins 3286
    [LightGBM] [Info] Number of data points in the train set: 3975, number of used features: 31
    [LightGBM] [Info] Start training from score 2.240503
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    [LightGBM] [Warning] No further splits with positive gain, best gain: -inf
    


    
![png](output_32_1.png)
    


    Graphs and evaluation results saved to ./プロ野球データ打撃成績_output/
    Test Data:
    Mean Squared Error (MSE): 16.930856698051937
    R-squared (R²): 0.4473656507732605
    Correlation Coefficient (r): 0.716142626188103
    
    Train Data:
    Mean Squared Error (MSE): 6.683825809524846
    R-squared (R²): 0.8080804395421426
    Correlation Coefficient (r): 0.9012850595952678
    


    
![png](output_32_3.png)
    



    
![png](output_32_4.png)
    

