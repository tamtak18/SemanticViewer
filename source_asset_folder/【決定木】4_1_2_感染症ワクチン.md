## 1.データ生成

参考　https://www.mhlw.go.jp/content/nenreikaikyubetsu-vaccination_data.pdf



```python
import numpy as np
import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.utils import resample
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
import os

# シードを固定して再現性を確保
np.random.seed(0)

# サンプルデータの生成
n_samples = 10000

# 年齢を20歳から80歳の範囲でランダムに生成
ages = np.random.randint(20, 80, n_samples)

# 年齢に基づいてワクチン接種の確率を設定
# 高齢者の方が接種率が高くなるよう調整（最大値は0.95）
vaccination_prob = np.clip((ages - 20) * 0.03, 0, 0.95)
vaccinated = np.random.binomial(1, vaccination_prob)

# 性別をランダムに生成 (0: 女性, 1: 男性)
gender = np.random.choice([0, 1], n_samples)

# 基礎疾患の有無をランダムに生成 (0: 無し, 1: 有り)
comorbidities = np.random.choice([0, 1], n_samples, p=[0.7, 0.3])

# 喫煙の有無をランダムに生成 (0: 非喫煙者, 1: 喫煙者)
smoking = np.random.choice([0, 1], n_samples, p=[0.8, 0.2])

# 肥満の有無をランダムに生成 (0: 非肥満, 1: 肥満)
obesity = np.random.choice([0, 1], n_samples, p=[0.7, 0.3])

# 酸素飽和度 (低酸素血症をランダムに生成, 0: 正常, 1: 低酸素血症)
hypoxia = np.random.choice([0, 1], n_samples, p=[0.9, 0.1])

# 年齢に基づいて重症化率を設定
# 年齢が高いほど重症化率が高くなるよう調整（最大値は0.2）
severity_rate_by_age = np.clip((ages - 20) * 0.005, 0, 0.2)

# 基礎疾患、喫煙、肥満、低酸素血症、性別の影響を加味した重症化率
severity_rate = severity_rate_by_age * (
    1 + 2 * comorbidities + 
    0.5 * smoking + 
    1.5 * obesity + 
    2 * hypoxia + 
    0.1 * gender
)

# ワクチン接種の効果を考慮した重症化率の調整
severity_rate *= (1 - 0.5 * vaccinated)

# 重症化率を0から1の範囲にクリップ
severity_rate = np.clip(severity_rate, 0, 1)

# 重症化したかどうかをランダムに決定
severe_cases = np.random.binomial(1, severity_rate)

# データフレームにまとめる
df = pd.DataFrame({
    'Age': ages,  # 年齢
    'Vaccinated': vaccinated,  # ワクチン接種の有無
    'Gender': gender,  # 性別
    'Comorbidities': comorbidities,  # 基礎疾患の有無
    'Smoking': smoking,  # 喫煙の有無
    'Obesity': obesity,  # 肥満の有無
    'Hypoxia': hypoxia,  # 低酸素血症の有無
    'Severe': severe_cases  # 重症化の有無
})

# 年齢層ごとにデータを層化
df['AgeGroup'] = pd.cut(df['Age'], bins=[19, 30, 40, 50, 60, 70, 80],
                labels=['20-30', '30-40', '40-50', '50-60', '60-70', '70-80'])

# データフレームをCSVファイルとして保存
df.to_csv('./decision_tree_input/generated_data_fixed.csv', index=False)

# データの確認
df.head()

```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Age</th>
      <th>Vaccinated</th>
      <th>Gender</th>
      <th>Comorbidities</th>
      <th>Smoking</th>
      <th>Obesity</th>
      <th>Hypoxia</th>
      <th>Severe</th>
      <th>AgeGroup</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>64</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>60-70</td>
    </tr>
    <tr>
      <th>1</th>
      <td>67</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>60-70</td>
    </tr>
    <tr>
      <th>2</th>
      <td>73</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>70-80</td>
    </tr>
    <tr>
      <th>3</th>
      <td>20</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>20-30</td>
    </tr>
    <tr>
      <th>4</th>
      <td>23</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>20-30</td>
    </tr>
  </tbody>
</table>
</div>



## 2. データの可視化


```python
output_dir = './decision_tree_output/'
# 各要因と重症化率の関係を可視化
factors = ['Comorbidities', 'Smoking', 'Obesity', 
            'Hypoxia', 'Gender','Vaccinated']

for factor in factors:
    plt.figure(figsize=(12, 6))
    factor_severity_rate = df.groupby(['AgeGroup', factor]) \
        ['Severe'].mean().reset_index()
    sns.barplot(x='AgeGroup', y='Severe', hue=factor, 
                data=factor_severity_rate, palette='viridis')
    plt.title(f'Severity Rate by {factor.capitalize()} and Age Group')
    plt.ylabel('Severity Rate')
    plt.xlabel('Age Group')
    filename = f"{output_dir}severity_rate_by_{factor.lower()}_and_age_group.svg"
    plt.savefig(filename, format='svg')
    plt.show()
    
# ワクチン接種の有無と重症化率の関係を年齢層を無視して可視化
severity_rate_by_vaccination = \
    df.groupby('Vaccinated')['Severe'].mean().reset_index()
plt.figure(figsize=(8, 6))
sns.barplot(x='Vaccinated', y='Severe', hue='Vaccinated', 
            data=severity_rate_by_vaccination, 
            palette='viridis', legend=False)
plt.title('Severity Rate by Vaccination Status (Without Age Stratification)')
plt.ylabel('Severity Rate')
filename = f"{output_dir}severity_rate_by_vaccination_status.svg"
plt.savefig(filename, format='svg')
plt.show()

```


    
![png](output_4_0.png)
    



    
![png](output_4_1.png)
    



    
![png](output_4_2.png)
    



    
![png](output_4_3.png)
    



    
![png](output_4_4.png)
    



    
![png](output_4_5.png)
    



    
![png](output_4_6.png)
    


## 3. 決定木モデルの作成と評価

### オーバーサンプリング 


```python
# AgeGroup列を削除
df = df.drop(columns=['AgeGroup'])

# 特徴量とターゲットの分離
X = df.drop('Severe', axis=1)
y = df['Severe']

# データの分割
X_train, X_test, y_train, y_test = train_test_split(X, y, 
                                    test_size=0.2, random_state=42)

# トレーニングデータで少数クラスのオーバーサンプリング
class_0 = X_train[y_train == 0]
class_1 = X_train[y_train == 1]
y_class_0 = y_train[y_train == 0]
y_class_1 = y_train[y_train == 1]

X_class_1_upsampled, y_class_1_upsampled = resample(
                        class_1,
                        y_class_1,
                        replace=True,  # サンプルを複製
                        n_samples=len(class_0),  # クラス0と同じ数にする
                        random_state=42)  # 乱数シード

X_train_balanced = pd.concat([class_0, X_class_1_upsampled])
y_train_balanced = pd.concat([y_class_0, y_class_1_upsampled])

```

### パイプライン構築


```python
# パイプラインの構築
pipeline = Pipeline([
    ('clf', DecisionTreeClassifier(random_state=42))
])

# ハイパーパラメータグリッド
param_grid = {
    'clf__criterion': ['gini', 'entropy'],
    'clf__splitter': ['best', 'random'],
    'clf__max_depth': [3, 5, 10],
    'clf__min_samples_split': [2, 10, 20],
    'clf__min_samples_leaf': [1, 5, 10]
}

# グリッドサーチCVの設定
grid_search_dt = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)

```

### モデルのトレーニング


```python

# モデルのトレーニング
grid_search_dt.fit(X_train_balanced, y_train_balanced)

# 最適なパラメータ
best_params = grid_search_dt.best_params_
print("Best Parameters:", best_params)

```

    Best Parameters: {'clf__criterion': 'gini', 'clf__max_depth': 10, 'clf__min_samples_leaf': 1, 'clf__min_samples_split': 2, 'clf__splitter': 'best'}
    

### 混同行列


```python
# 最良のモデル
best_clf_dt = grid_search_dt.best_estimator_

# 最良のモデルによる予測値
y_pred_test = best_clf_dt.predict(X_test)
# モデルの評価
conf_matrix_dt = confusion_matrix(y_test, y_pred_test)
accuracy_dt = accuracy_score(y_test, y_pred_test)
report_dt = classification_report(y_test, y_pred_test, digits=3)

# ヒートマップに表示するための注釈を作成
labels = np.array([['TN={}'.format(conf_matrix_dt[0, 0]), 'FP={}'.format(conf_matrix_dt[0, 1])],
                ['FN={}'.format(conf_matrix_dt[1, 0]), 'TP={}'.format(conf_matrix_dt[1, 1])]])

# 混同行列をヒートマップとして表示
print('Decision Tree Confusion Matrix:')
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_dt, annot=labels, fmt='', cmap='Wistia', cbar=False, 
            xticklabels=['Non-Severe', 'Severe'], yticklabels=['Non-Severe', 'Severe'], 
            annot_kws={"color": "black", "fontsize": 16})
plt.xticks(fontsize=16)
plt.yticks(fontsize=16)
plt.xlabel('Predicted', fontsize=16)
plt.ylabel('Actual', fontsize=16)
plt.title('Decision Tree Confusion Matrix', fontsize=16)
# SVG形式で保存
confusion_matrix_svg_path = os.path.join(output_dir, "confusion_matrix.svg")
plt.savefig(confusion_matrix_svg_path, format='svg')
print(f"Confusion matrix saved to: {confusion_matrix_svg_path}")
plt.show()

```

    Decision Tree Confusion Matrix:
    Confusion matrix saved to: ./decision_tree_output/confusion_matrix.svg
    


    
![png](output_13_1.png)
    



```python
output_dir
```




    './decision_tree_output/'



### 性能評価指標


```python

# 性能評価を表示
print(f'Decision Tree Accuracy: {accuracy_dt:.3f}')

# print(conf_matrix_dt)
print('Decision Tree Classification Report:')
print(report_dt)

```

    Decision Tree Accuracy: 0.667
    Decision Tree Classification Report:
                  precision    recall  f1-score   support
    
               0      0.901     0.667     0.767      1639
               1      0.307     0.668     0.420       361
    
        accuracy                          0.667      2000
       macro avg      0.604     0.668     0.594      2000
    weighted avg      0.794     0.667     0.704      2000
    
    

### 特徴量の重要度


```python


# 特徴量名と重要度を取得
feature_importances = best_clf_dt.named_steps['clf'].feature_importances_
feature_names = X_train_balanced.columns  # オーバーサンプリング後のX_trainを使用

# 特徴量名と重要度を統一
importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': np.pad(feature_importances, (0, len(feature_names) - len(feature_importances)), constant_values=0)
}).sort_values(by='Importance', ascending=False)

# 特徴量の重要度を棒グラフとして表示
plt.figure(figsize=(10, 6))
plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')
plt.xlabel('Importance', fontsize=14)
plt.ylabel('Feature', fontsize=14)
plt.title('Feature Importance from Decision Tree', fontsize=16)
plt.gca().invert_yaxis()
plt.tight_layout()

# 棒グラフをSVG形式で保存
feature_importance_svg = os.path.join(output_dir, 'feature_importance.svg')
plt.savefig(feature_importance_svg, format='svg')
plt.show()
```


    
![png](output_18_0.png)
    



```python
feature_names
```




    Index(['Age', 'Vaccinated', 'Gender', 'Comorbidities', 'Smoking', 'Obesity',
           'Hypoxia'],
          dtype='object')



### 木の深さと正解率


```python
from sklearn.tree import DecisionTreeClassifier, export_graphviz
from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, ConfusionMatrixDisplay, accuracy_score
import graphviz

# スコアを保存するリスト
evaluation_metrics = []
training_accuracies = []
test_accuracies = []

# # 保存先フォルダを作成
# output_dir = "./決定木可視化"
# os.makedirs(output_dir, exist_ok=True)

# max_depth を 1 から 11 まで試す
max_depth_range = range(1, 15)

for max_depth in max_depth_range:
    # 決定木の設定
    tree = DecisionTreeClassifier(criterion='gini', 
            max_depth=max_depth, min_samples_leaf=1, min_samples_split=2, splitter='best', random_state=42)

    # 決定木を構築
    tree.fit(X_train_balanced, y_train_balanced)

    # トレーニングデータとテストデータの予測
    train_pred = tree.predict(X_train_balanced)
    test_pred = tree.predict(X_test)

    # Accuracy の計算
    train_accuracy = accuracy_score(y_train_balanced, train_pred)
    test_accuracy = accuracy_score(y_test, test_pred)

    # 精度評価指標を計算
    precision, recall, f1, _ = precision_recall_fscore_support(y_test, test_pred, average='weighted')

    evaluation_metrics.append({
        'max_depth': max_depth,
        'accuracy': test_accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1
    })

    # Accuracy を保存
    training_accuracies.append(train_accuracy)
    test_accuracies.append(test_accuracy)
    
        # 決定木の可視化と保存
    tree_dot = export_graphviz(
        tree, 
        out_file=None, 
        feature_names=X_train_balanced.columns, 
        class_names=[str(cls) for cls in tree.classes_], 
        filled=True, 
        rounded=True, 
        special_characters=True
    )
    tree_graph = graphviz.Source(tree_dot)
    tree_path = os.path.join(output_dir, f"decision_tree_depth{max_depth}")
    tree_graph.render(tree_path, format="svg", cleanup=True)
    print(f"Decision tree visualization saved: {tree_path}")

    # 混同行列の計算と保存
    cm = confusion_matrix(y_test, test_pred, labels=y.unique())
    fig, ax = plt.subplots(figsize=(6, 6))
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=y.unique())
    disp.plot(cmap="Greys", ax=ax, colorbar=False)
    plt.title(f"Confusion Matrix (Max Depth={max_depth})")
    plt.tight_layout()
    confusion_matrix_path = os.path.join(output_dir, f"confusion_matrix_depth{max_depth}.svg")
    plt.savefig(confusion_matrix_path, format="svg")
    plt.close(fig)
    print(f"Confusion matrix saved: {confusion_matrix_path}")

# Accuracy を折れ線グラフでプロット
accuracy_plot_path = os.path.join(output_dir, "accuracy_vs_max_depth.svg")
plt.figure(figsize=(8, 5))
plt.plot(max_depth_range, training_accuracies, label="Training Accuracy", color="black", marker="o")
plt.plot(max_depth_range, test_accuracies, label="Test Accuracy", color="gray", linestyle="--", marker="o")
plt.title("Accuracy vs. Max Depth")
plt.xlabel("Max Depth")
plt.ylabel("Accuracy")
plt.xticks(max_depth_range)
plt.legend()
plt.grid(True)
plt.savefig(accuracy_plot_path, format="svg")
plt.show()
print(f"Accuracy plot saved as SVG: {accuracy_plot_path}")

# 評価指標をDataFrameに変換
metrics_df = pd.DataFrame(evaluation_metrics)

# xlsxファイルとして保存
metrics_path = os.path.join(output_dir, "evaluation_metrics.xlsx")
metrics_df.to_excel(metrics_path, index=False)
print(f"Evaluation metrics saved as Excel: {metrics_path}")

# 表をコンソールに表示
print("Evaluation Metrics by Max Depth:")
print(metrics_df)

```

    Decision tree visualization saved: ./decision_tree_output/decision_tree_depth1
    Confusion matrix saved: ./decision_tree_output/confusion_matrix_depth1.svg
    Decision tree visualization saved: ./decision_tree_output/decision_tree_depth2
    Confusion matrix saved: ./decision_tree_output/confusion_matrix_depth2.svg
    Decision tree visualization saved: ./decision_tree_output/decision_tree_depth3
    Confusion matrix saved: ./decision_tree_output/confusion_matrix_depth3.svg
    Decision tree visualization saved: ./decision_tree_output/decision_tree_depth4
    Confusion matrix saved: ./decision_tree_output/confusion_matrix_depth4.svg
    Decision tree visualization saved: ./decision_tree_output/decision_tree_depth5
    Confusion matrix saved: ./decision_tree_output/confusion_matrix_depth5.svg
    Decision tree visualization saved: ./decision_tree_output/decision_tree_depth6
    Confusion matrix saved: ./decision_tree_output/confusion_matrix_depth6.svg
    Decision tree visualization saved: ./decision_tree_output/decision_tree_depth7
    Confusion matrix saved: ./decision_tree_output/confusion_matrix_depth7.svg
    Decision tree visualization saved: ./decision_tree_output/decision_tree_depth8
    Confusion matrix saved: ./decision_tree_output/confusion_matrix_depth8.svg
    Decision tree visualization saved: ./decision_tree_output/decision_tree_depth9
    Confusion matrix saved: ./decision_tree_output/confusion_matrix_depth9.svg
    Decision tree visualization saved: ./decision_tree_output/decision_tree_depth10
    Confusion matrix saved: ./decision_tree_output/confusion_matrix_depth10.svg
    Decision tree visualization saved: ./decision_tree_output/decision_tree_depth11
    Confusion matrix saved: ./decision_tree_output/confusion_matrix_depth11.svg
    Decision tree visualization saved: ./decision_tree_output/decision_tree_depth12
    Confusion matrix saved: ./decision_tree_output/confusion_matrix_depth12.svg
    Decision tree visualization saved: ./decision_tree_output/decision_tree_depth13
    Confusion matrix saved: ./decision_tree_output/confusion_matrix_depth13.svg
    Decision tree visualization saved: ./decision_tree_output/decision_tree_depth14
    Confusion matrix saved: ./decision_tree_output/confusion_matrix_depth14.svg
    


    
![png](output_21_1.png)
    


    Accuracy plot saved as SVG: ./decision_tree_output/accuracy_vs_max_depth.svg
    Evaluation metrics saved as Excel: ./decision_tree_output/evaluation_metrics.xlsx
    Evaluation Metrics by Max Depth:
        max_depth  accuracy  precision  recall  f1_score
    0           1    0.3440   0.798662  0.3440  0.348127
    1           2    0.7395   0.777043  0.7395  0.754926
    2           3    0.6475   0.787109  0.6475  0.687339
    3           4    0.6345   0.799043  0.6345  0.676629
    4           5    0.6460   0.791250  0.6460  0.686289
    5           6    0.6780   0.799234  0.6780  0.713349
    6           7    0.6640   0.793515  0.6640  0.701436
    7           8    0.6615   0.800576  0.6615  0.699791
    8           9    0.6775   0.796597  0.6775  0.712716
    9          10    0.6675   0.793839  0.6675  0.704335
    10         11    0.6825   0.785264  0.6825  0.715471
    11         12    0.6795   0.774955  0.6795  0.711697
    12         13    0.6850   0.771466  0.6850  0.715265
    13         14    0.6820   0.767100  0.6820  0.712238
    

### 参考　オーバーサンプリングなし


```python
# オーバーサンプリングなし

from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report
import pandas as pd

# データの読み込み
data = pd.read_csv('./decision_tree_output/generated_data_fixed.csv')  # ファイルパスを修正してください

# AgeGroup列を削除
data = data.drop(columns=['AgeGroup'])

# 特徴量とターゲットの分離
X = data.drop('Severe', axis=1)
y = data['Severe']

# データの分割
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# パイプラインの構築
pipeline = Pipeline([
    ('scaler', StandardScaler()),  # 標準化
    ('clf', DecisionTreeClassifier(random_state=42))
])

# ハイパーパラメータグリッド
param_grid = {
    'clf__criterion': ['gini', 'entropy'],
    'clf__splitter': ['best', 'random'],
    'clf__max_depth': [3, 5, 10],
    'clf__min_samples_split': [2, 10, 20],
    'clf__min_samples_leaf': [1, 5, 10]
}

# グリッドサーチCVの設定
grid_search_dt = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)

# モデルのトレーニング
grid_search_dt.fit(X_train, y_train)

# 最適なパラメータ
best_params = grid_search_dt.best_params_

# 最良のモデル
best_clf_dt = grid_search_dt.best_estimator_

# テストデータに対する評価
y_pred_test = best_clf_dt.predict(X_test)

# 評価結果
best_accuracy = accuracy_score(y_test, y_pred_test)
best_report = classification_report(y_test, y_pred_test)

print("Best Parameters:", best_params)
print("Accuracy on Test Data:", best_accuracy)
print("Classification Report on Test Data:\n", best_report)

```

    Best Parameters: {'clf__criterion': 'gini', 'clf__max_depth': 3, 'clf__min_samples_leaf': 1, 'clf__min_samples_split': 2, 'clf__splitter': 'random'}
    Accuracy on Test Data: 0.8245
    Classification Report on Test Data:
                   precision    recall  f1-score   support
    
               0       0.83      0.99      0.90      1639
               1       0.68      0.05      0.10       361
    
        accuracy                           0.82      2000
       macro avg       0.75      0.52      0.50      2000
    weighted avg       0.80      0.82      0.76      2000
    
    


```python
# モデルの評価
accuracy_dt = accuracy_score(y_test, y_pred_test)
conf_matrix_dt = confusion_matrix(y_test, y_pred_test)
report_dt = classification_report(y_test, y_pred_test, digits=3)

# 結果を表示
print(f'Decision Tree Accuracy: {accuracy_dt:.3f}')

# print(conf_matrix_dt)
print('Decision Tree Classification Report:')
print(report_dt)



# 混同行列をヒートマップとして表示
print('Decision Tree Confusion Matrix:')
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_dt, annot=True, fmt='d', cmap='Wistia', cbar=False, xticklabels=['Non-Severe', 'Severe'], yticklabels=['Non-Severe', 'Severe'], annot_kws={"color": "black", "fontsize": 16})
plt.xticks(fontsize=16)
plt.yticks(fontsize=16)
plt.xlabel('Predicted', fontsize=16)
plt.ylabel('Actual', fontsize=16)
plt.title('Decision Tree Confusion Matrix', fontsize=16)
plt.show()
```

    Decision Tree Accuracy: 0.825
    Decision Tree Classification Report:
                  precision    recall  f1-score   support
    
               0      0.827     0.995     0.903      1639
               1      0.679     0.053     0.098       361
    
        accuracy                          0.825      2000
       macro avg      0.753     0.524     0.500      2000
    weighted avg      0.800     0.825     0.757      2000
    
    Decision Tree Confusion Matrix:
    


    
![png](output_24_1.png)
    



```python
from sklearn.tree import export_graphviz
import graphviz

# 決定木のエクスポート
dot_data = export_graphviz(
    best_clf_dt.named_steps['clf'],
    out_file=None,
    feature_names=X_train.columns,
    class_names=['Non-Severe', 'Severe'],
    filled=True,
    rounded=True,
    special_characters=True
)

# DOT形式をGraphvizで可視化
graph = graphviz.Source(dot_data)

# SVG形式で保存
graph.render(os.path.join(output_dir, "decision_tree"), format='svg')

print(f"Decision tree saved to: {output_dir}/decision_tree.svg")


```

    Decision tree saved to: ./decision_tree_output//decision_tree.svg
    


```python
from dtreeviz.trees import dtreeviz

# dtreevizによる可視化
viz = dtreeviz(
    best_clf_dt.named_steps['clf'], 
    X_train, 
    y_train,
    target_name='Severe',
    feature_names=X_train.columns,
    class_names=['Non-Severe', 'Severe']
)

# SVG形式で保存
viz.save("decision_tree.svg")

# 可視化の表示
viz

# # 決定木の可視化 (dtreeviz)
# viz = dtreeviz(best_clf_dt, X_train, y_train,
#                target_name="Severity",
#                feature_names=X_train.columns,
#                class_names=["Non-Severe", "Severe"])

# viz

# # 決定木の可視化 (graphviz)
# dot_data = export_graphviz(best_clf_dt, out_file=None, 
#                            feature_names=X.columns,  
#                            class_names=['Non-Severe', 'Severe'],  
#                            filled=True, rounded=True,  
#                            special_characters=True)  
# graph = graphviz.Source(dot_data)  
# graph.render("decision_tree")

# # 決定木の可視化 (dtreeviz)
# viz = dtreeviz(best_clf_dt, X_train, y_train,
#                target_name="Severity",
#                feature_names=X.columns,
#                class_names=["Non-Severe", "Severe"])

# viz.save("decision_tree_viz.svg")
# viz.view()

# # 決定木の可視化 (graphviz)
# dot_data = export_graphviz(best_clf_dt, out_file=None, 
#                            feature_names=X.columns,  
#                            class_names=['Non-Severe', 'Severe'],  
#                            filled=True, rounded=True,  
#                            special_characters=True)  
# graph = graphviz.Source(dot_data)  
# graph.render("decision_tree")

# # 決定木の可視化 (dtreeviz)
# viz = dtreeviz(best_clf_dt, X_train, y_train,
#                target_name="Severity",
#                feature_names=X.columns,
#                class_names=["Non-Severe", "Severe"])

# viz.save("decision_tree_viz.svg")
# viz.view()
```




    
![svg](output_26_0.svg)
    



## LightGBMモデル


```python
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.utils import resample
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report
import pandas as pd
import lightgbm as lgb

# データの読み込み
data = pd.read_csv('./decision_tree_output/generated_data_fixed.csv')  # ファイルパスを修正してください

# AgeGroup列を削除
data = data.drop(columns=['AgeGroup'])

# 特徴量とターゲットの分離
X = data.drop('Severe', axis=1)
y = data['Severe']

# データの分割
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 訓練用データで少数クラスのオーバーサンプリング
class_0 = X_train[y_train == 0]
class_1 = X_train[y_train == 1]
y_class_0 = y_train[y_train == 0]
y_class_1 = y_train[y_train == 1]

X_class_1_upsampled, y_class_1_upsampled = resample(class_1,
                                                    y_class_1,
                                                    replace=True,  # サンプルを複製
                                                    n_samples=len(class_0),  # クラス0と同じ数にする
                                                    random_state=42)  # 乱数シード

X_train_balanced = pd.concat([class_0, X_class_1_upsampled])
y_train_balanced = pd.concat([y_class_0, y_class_1_upsampled])

# パイプラインの構築
pipeline = Pipeline([
    ('scaler', StandardScaler()),  # 標準化
    ('clf', lgb.LGBMClassifier(random_state=42))
])

# ハイパーパラメータグリッド
param_grid = {
    'clf__num_leaves': [31, 50, 100],
    'clf__learning_rate': [0.1, 0.01, 0.001],
    'clf__n_estimators': [100, 200, 500]
}

# グリッドサーチCVの設定
grid_search_lgb = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)

# モデルのトレーニング
grid_search_lgb.fit(X_train_balanced, y_train_balanced)

# 最適なパラメータ
best_params = grid_search_lgb.best_params_

# 最良のモデル
best_clf_lgb = grid_search_lgb.best_estimator_

# テストデータに対する評価
y_pred_test = best_clf_lgb.predict(X_test)

# 評価結果
best_accuracy = accuracy_score(y_test, y_pred_test)
best_report = classification_report(y_test, y_pred_test)

print("Best Parameters:", best_params)
print("Accuracy on Test Data:", best_accuracy)
print("Classification Report on Test Data:\n", best_report)


# from sklearn.model_selection import train_test_split, GridSearchCV
# import lightgbm as lgb


# # 特徴量とターゲットに分ける
# X = df[['Age', 'Vaccinated', 'Gender', 'Comorbidities', 'Smoking', 'Obesity', 'Hypoxia']]
# y = df['Severe']

# # データをトレーニングセットとテストセットに分ける
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# # LightGBMモデルのハイパーパラメータチューニング
# param_grid = {
#     'num_leaves': [31, 50, 70],
#     'max_depth': [-1, 10, 20],
#     'learning_rate': [0.01, 0.1, 0.2],
#     'n_estimators': [100, 200, 500]
# }
# clf_lgb = lgb.LGBMClassifier(random_state=42)
# grid_search_lgb = GridSearchCV(clf_lgb, param_grid, scoring='f1', cv=5)
# grid_search_lgb.fit(X_train, y_train)

# # 最良モデルで予測
# best_clf_lgb = grid_search_lgb.best_estimator_
# y_pred_lgb = best_clf_lgb.predict(X_test)

# # モデルの評価
# accuracy_lgb = accuracy_score(y_test, y_pred_lgb)
# conf_matrix_lgb = confusion_matrix(y_test, y_pred_lgb)
# report_lgb = classification_report(y_test, y_pred_lgb)

# # 結果を表示
# print(f'LightGBM Accuracy: {accuracy_lgb:.2f}')
# print('LightGBM Confusion Matrix:')
# print(conf_matrix_lgb)
# print('LightGBM Classification Report:')
# print(report_lgb)

# # 混同行列をヒートマップとして表示
# plt.figure(figsize=(8, 6))
# sns.heatmap(conf_matrix_lgb, annot=True, fmt='d', cmap='viridis', cbar=False, xticklabels=['Non-Severe', 'Severe'], yticklabels=['Non-Severe', 'Severe'], annot_kws={"color": "black"})
# plt.xlabel('Predicted')
# plt.ylabel('Actual')
# plt.title('LightGBM Confusion Matrix')
# plt.show()

# # # 予測結果と実際の重症化率を比較する散布図
# # df_test_lgb = X_test.copy()
# # df_test_lgb['TrueSeverity'] = y_test
# # df_test_lgb['PredictedSeverity'] = y_pred_lgb

# # plt.figure(figsize=(12, 6))
# # sns.scatterplot(x='Age', y='TrueSeverity', data=df_test_lgb, hue='Vaccinated', style='PredictedSeverity', palette='viridis', markers=['o', 'X'])
# # plt.title('True vs Predicted Severity by Age and Vaccination Status (LightGBM)')
# # plt.xlabel('Age')
# # plt.ylabel('Severity (0: Non-Severe, 1: Severe)')
# # plt.legend(title='Vaccinated', loc='upper left', labels=['Not Vaccinated, Predicted', 'Not Vaccinated, True', 'Vaccinated, Predicted', 'Vaccinated, True'])
# # plt.show()

```

    [LightGBM] [Info] Number of positive: 6542, number of negative: 6542
    [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000107 seconds.
    You can set `force_row_wise=true` to remove the overhead.
    And if memory is not enough, you can set `force_col_wise=true`.
    [LightGBM] [Info] Total Bins 79
    [LightGBM] [Info] Number of data points in the train set: 13084, number of used features: 7
    [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
    Best Parameters: {'clf__learning_rate': 0.1, 'clf__n_estimators': 500, 'clf__num_leaves': 100}
    Accuracy on Test Data: 0.6755
    Classification Report on Test Data:
                   precision    recall  f1-score   support
    
               0       0.87      0.71      0.78      1639
               1       0.28      0.51      0.36       361
    
        accuracy                           0.68      2000
       macro avg       0.57      0.61      0.57      2000
    weighted avg       0.76      0.68      0.71      2000
    
    


```python
# モデルの評価
accuracy_dt = accuracy_score(y_test, y_pred_test)
conf_matrix_dt = confusion_matrix(y_test, y_pred_test)
report_dt = classification_report(y_test, y_pred_test, digits=3)

# 結果を表示
print(f'Decision Tree Accuracy: {accuracy_dt:.3f}')

# print(conf_matrix_dt)
print('Decision Tree Classification Report:')
print(report_dt)



# 混同行列をヒートマップとして表示
print('Decision Tree Confusion Matrix:')
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_dt, annot=True, fmt='d', cmap='Wistia', cbar=False, xticklabels=['Non-Severe', 'Severe'], yticklabels=['Non-Severe', 'Severe'], annot_kws={"color": "black", "fontsize": 16})
plt.xticks(fontsize=16)
plt.yticks(fontsize=16)
plt.xlabel('Predicted', fontsize=16)
plt.ylabel('Actual', fontsize=16)
plt.title('Decision Tree Confusion Matrix', fontsize=16)
plt.show()
```

    Decision Tree Accuracy: 0.675
    Decision Tree Classification Report:
                  precision    recall  f1-score   support
    
               0      0.868     0.713     0.783      1639
               1      0.280     0.507     0.361       361
    
        accuracy                          0.675      2000
       macro avg      0.574     0.610     0.572      2000
    weighted avg      0.762     0.675     0.706      2000
    
    Decision Tree Confusion Matrix:
    


    
![png](output_29_1.png)
    



```python
# オーバーサンプリングなし

from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report
import pandas as pd
import lightgbm as lgb

# データの読み込み
data = pd.read_csv('./decision_tree_output/generated_data_fixed.csv')  # ファイルパスを修正してください

# AgeGroup列を削除
data = data.drop(columns=['AgeGroup'])

# 特徴量とターゲットの分離
X = data.drop('Severe', axis=1)
y = data['Severe']

# データの分割
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# パイプラインの構築
pipeline = Pipeline([
    ('scaler', StandardScaler()),  # 標準化
    ('clf', lgb.LGBMClassifier(random_state=42))
])

# ハイパーパラメータグリッド
param_grid = {
    'clf__num_leaves': [31, 50, 100],
    'clf__learning_rate': [0.1, 0.01, 0.001],
    'clf__n_estimators': [100, 200, 500]
}

# グリッドサーチCVの設定
grid_search_lgb = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)

# モデルのトレーニング
grid_search_lgb.fit(X_train, y_train)

# 最適なパラメータ
best_params = grid_search_lgb.best_params_

# 最良のモデル
best_clf_lgb = grid_search_lgb.best_estimator_

# テストデータに対する評価
y_pred_test = best_clf_lgb.predict(X_test)

# 評価結果
best_accuracy = accuracy_score(y_test, y_pred_test)
best_report = classification_report(y_test, y_pred_test)

print("Best Parameters:", best_params)
print("Accuracy on Test Data:", best_accuracy)
print("Classification Report on Test Data:\n", best_report)

```

    [LightGBM] [Info] Number of positive: 1458, number of negative: 6542
    [LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000590 seconds.
    You can set `force_row_wise=true` to remove the overhead.
    And if memory is not enough, you can set `force_col_wise=true`.
    [LightGBM] [Info] Total Bins 79
    [LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 7
    [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.182250 -> initscore=-1.501177
    [LightGBM] [Info] Start training from score -1.501177
    Best Parameters: {'clf__learning_rate': 0.01, 'clf__n_estimators': 200, 'clf__num_leaves': 50}
    Accuracy on Test Data: 0.822
    Classification Report on Test Data:
                   precision    recall  f1-score   support
    
               0       0.83      0.99      0.90      1639
               1       0.56      0.07      0.12       361
    
        accuracy                           0.82      2000
       macro avg       0.69      0.53      0.51      2000
    weighted avg       0.78      0.82      0.76      2000
    
    


```python
# モデルの評価
accuracy_dt = accuracy_score(y_test, y_pred_test)
conf_matrix_dt = confusion_matrix(y_test, y_pred_test)
report_dt = classification_report(y_test, y_pred_test, digits=3)

# 結果を表示
print(f'Decision Tree Accuracy: {accuracy_dt:.3f}')

# print(conf_matrix_dt)
print('Decision Tree Classification Report:')
print(report_dt)



# 混同行列をヒートマップとして表示
print('Decision Tree Confusion Matrix:')
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_dt, annot=True, fmt='d', cmap='Wistia', cbar=False, xticklabels=['Non-Severe', 'Severe'], yticklabels=['Non-Severe', 'Severe'], annot_kws={"color": "black", "fontsize": 16})
plt.xticks(fontsize=16)
plt.yticks(fontsize=16)
plt.xlabel('Predicted', fontsize=16)
plt.ylabel('Actual', fontsize=16)
plt.title('Decision Tree Confusion Matrix', fontsize=16)
plt.show()
```

    Decision Tree Accuracy: 0.822
    Decision Tree Classification Report:
                  precision    recall  f1-score   support
    
               0      0.828     0.988     0.901      1639
               1      0.558     0.066     0.119       361
    
        accuracy                          0.822      2000
       macro avg      0.693     0.527     0.510      2000
    weighted avg      0.779     0.822     0.760      2000
    
    Decision Tree Confusion Matrix:
    


    
![png](output_31_1.png)
    


## XGBoost


```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score
import xgboost as xgb

# シードを固定して再現性を確保
np.random.seed(0)

# サンプルデータの生成
n_samples = 10000

# 年齢を20歳から80歳の範囲でランダムに生成
ages = np.random.randint(20, 80, n_samples)

# 年齢に基づいてワクチン接種の確率を設定（高齢者の方がワクチン接種率が非常に高い）
vaccination_prob = np.clip((ages - 20) * 0.03, 0, 0.95)
vaccinated = np.random.binomial(1, vaccination_prob)

# 性別をランダムに生成 (0: 女性, 1: 男性)
gender = np.random.choice([0, 1], n_samples)

# 基礎疾患の有無をランダムに生成 (0: 無し, 1: 有り)
comorbidities = np.random.choice([0, 1], n_samples, p=[0.7, 0.3])

# 喫煙の有無をランダムに生成 (0: 非喫煙者, 1: 喫煙者)
smoking = np.random.choice([0, 1], n_samples, p=[0.8, 0.2])

# 肥満の有無をランダムに生成 (0: 非肥満, 1: 肥満)
obesity = np.random.choice([0, 1], n_samples, p=[0.7, 0.3])

# 酸素飽和度 (低酸素血症をランダムに生成, 0: 正常, 1: 低酸素血症)
hypoxia = np.random.choice([0, 1], n_samples, p=[0.9, 0.1])

# 年齢に基づいて重症化率を設定
severity_rate_by_age = np.clip((ages - 20) * 0.01, 0, 0.5)

# 基礎疾患、喫煙、肥満、低酸素血症、性別の影響を加味した重症化率
severity_rate = severity_rate_by_age * (
    1 - 0.8 * (1 - comorbidities) + 0.8 * comorbidities + 
    1 - 0.5 * (1 - smoking) + 0.5 * smoking + 
    1 - 0.8 * (1 - obesity) + 0.8 * obesity + 
    1 - 0.5 * (1 - hypoxia) + 0.5 * hypoxia + 
    0.2 * gender
)

# ワクチン接種の効果を考慮した重症化率の調整
severity_rate *= (1 - 0.5 * vaccinated)

# 重症化率を0から1の範囲にクリップ
severity_rate = np.clip(severity_rate, 0, 1)

# 重症化したかどうかをランダムに決定
severe_cases = np.random.binomial(1, severity_rate)

# データフレームにまとめる
df = pd.DataFrame({
    'Age': ages,
    'Vaccinated': vaccinated,
    'Gender': gender,
    'Comorbidities': comorbidities,
    'Smoking': smoking,
    'Obesity': obesity,
    'Hypoxia': hypoxia,
    'Severe': severe_cases
})

# データの可視化
plt.figure(figsize=(12, 6))
sns.histplot(data=df, x='Age', hue='Severe', multiple='stack', palette='viridis')
plt.title('Age Distribution by Severity')
plt.show()

# ワクチン接種の有無と重症化率の関係を年齢層を無視して可視化
severity_rate_by_vaccination = df.groupby('Vaccinated')['Severe'].mean().reset_index()
plt.figure(figsize=(8, 6))
sns.barplot(x='Vaccinated', y='Severe', data=severity_rate_by_vaccination, palette='viridis')
plt.title('Severity Rate by Vaccination Status (Without Age Stratification)')
plt.show()

# 年齢層ごとのワクチン接種の有無と重症化率の関係を可視化
df['AgeGroup'] = pd.cut(df['Age'], bins=[20, 30, 40, 50, 60, 70, 80], labels=['20-30', '30-40', '40-50', '50-60', '60-70', '70-80'])
severity_rate_by_age_vaccination = df.groupby(['AgeGroup', 'Vaccinated'])['Severe'].mean().reset_index()
plt.figure(figsize=(12, 6))
sns.lineplot(x='AgeGroup', y='Severe', hue='Vaccinated', data=severity_rate_by_age_vaccination, marker='o', palette='viridis')
plt.title('Severity Rate by Age Group and Vaccination Status')
plt.show()

# クロス集計表の作成
cross_tab = pd.crosstab(df['AgeGroup'], df['Vaccinated'], values=df['Severe'], aggfunc='mean')
print("Cross Tabulation of Severity Rate by Age Group and Vaccination Status:")
print(cross_tab)

# 特徴量とターゲットに分ける
X = df[['Age', 'Vaccinated', 'Gender', 'Comorbidities', 'Smoking', 'Obesity', 'Hypoxia']]
y = df['Severe']

# データをトレーニングセットとテストセットに分ける
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# XGBoostモデルのハイパーパラメータチューニング
param_grid = {
    'max_depth': [3, 5, 7, 10],
    'learning_rate': [0.01, 0.1, 0.2],
    'n_estimators': [100, 200, 500],
    'subsample': [0.8, 1.0]
}
clf_xgb = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')
grid_search_xgb = GridSearchCV(clf_xgb, param_grid, scoring='f1', cv=5)
grid_search_xgb.fit(X_train, y_train)

# 最良モデルで予測
best_clf_xgb = grid_search_xgb.best_estimator_
y_pred_xgb = best_clf_xgb.predict(X_test)

# モデルの評価
accuracy_xgb = accuracy_score(y_test, y_pred_xgb)
conf_matrix_xgb = confusion_matrix(y_test, y_pred_xgb)
report_xgb = classification_report(y_test, y_pred_xgb)

# 結果を表示
print(f'XGBoost Accuracy: {accuracy_xgb:.2f}')
print('XGBoost Confusion Matrix:')
print(conf_matrix_xgb)
print('XGBoost Classification Report:')
print(report_xgb)

# 混同行列をヒートマップとして表示
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_xgb, annot=True, fmt='d', cmap='viridis', cbar=False, xticklabels=['Non-Severe', 'Severe'], yticklabels=['Non-Severe', 'Severe'], annot_kws={"color": "black"})
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('XGBoost Confusion Matrix')
plt.show()

# 予測結果と実際の重症化率を比較する散布図
df_test_xgb = X_test.copy()
df_test_xgb['TrueSeverity'] = y_test
df_test_xgb['PredictedSeverity'] = y_pred_xgb

plt.figure(figsize=(12, 6))
sns.scatterplot(x='Age', y='TrueSeverity', data=df_test_xgb, hue='Vaccinated', style='PredictedSeverity', palette='viridis', markers=['o', 'X'])
plt.title('True vs Predicted Severity by Age and Vaccination Status (XGBoost)')
plt.xlabel('Age')
plt.ylabel('Severity (0: Non-Severe, 1: Severe)')
plt.legend(title='Vaccinated', loc='upper left', labels=['Not Vaccinated, Predicted', 'Not Vaccinated, True', 'Vaccinated, Predicted', 'Vaccinated, True'])
plt.show()
```


    
![png](output_33_0.png)
    


    C:\Users\tamta\AppData\Local\Temp\ipykernel_28192\3625012845.py:79: FutureWarning: 
    
    Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.
    
      sns.barplot(x='Vaccinated', y='Severe', data=severity_rate_by_vaccination, palette='viridis')
    


    
![png](output_33_2.png)
    



    
![png](output_33_3.png)
    


    Cross Tabulation of Severity Rate by Age Group and Vaccination Status:
    Vaccinated         0         1
    AgeGroup                      
    20-30       0.155050  0.081181
    30-40       0.432609  0.227794
    40-50       0.641711  0.378522
    50-60       0.804124  0.476730
    60-70       0.924051  0.609288
    70-80       0.950820  0.664275
    XGBoost Accuracy: 0.74
    XGBoost Confusion Matrix:
    [[1380  296]
     [ 480  844]]
    XGBoost Classification Report:
                  precision    recall  f1-score   support
    
               0       0.74      0.82      0.78      1676
               1       0.74      0.64      0.69      1324
    
        accuracy                           0.74      3000
       macro avg       0.74      0.73      0.73      3000
    weighted avg       0.74      0.74      0.74      3000
    
    


    
![png](output_33_5.png)
    



    
![png](output_33_6.png)
    


## 各要因と重症化率の関係(年齢で層化）


```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 生成したデータを再利用
# (この部分は以前のコードと同じです)
np.random.seed(0)
n_samples = 10000
ages = np.random.randint(20, 80, n_samples)
vaccination_prob = np.clip((ages - 20) * 0.03, 0, 0.98)
vaccinated = np.random.binomial(1, vaccination_prob)
gender = np.random.choice([0, 1], n_samples)
comorbidities = np.random.choice([0, 1], n_samples, p=[0.7, 0.3])
smoking = np.random.choice([0, 1], n_samples, p=[0.8, 0.2])
obesity = np.random.choice([0, 1], n_samples, p=[0.7, 0.3])
hypoxia = np.random.choice([0, 1], n_samples, p=[0.9, 0.1])
severity_rate_by_age = np.clip((ages - 20) * 0.01, 0, 0.5)
severity_rate = severity_rate_by_age * (
    1 - 0.8 * (1 - comorbidities) + 0.8 * comorbidities + 
    1 - 0.5 * (1 - smoking) + 0.5 * smoking + 
    1 - 0.8 * (1 - obesity) + 0.8 * obesity + 
    1 - 0.5 * (1 - hypoxia) + 0.5 * hypoxia + 
    0.2 * gender
)
severity_rate *= (1 - 0.5 * vaccinated)
severity_rate = np.clip(severity_rate, 0, 1)
severe_cases = np.random.binomial(1, severity_rate)

df = pd.DataFrame({
    'Age': ages,  # 年齢
    'Vaccinated': vaccinated,  # ワクチン接種の有無
    'Gender': gender,  # 性別
    'Comorbidities': comorbidities,  # 基礎疾患の有無
    'Smoking': smoking,  # 喫煙の有無
    'Obesity': obesity,  # 肥満の有無
    'Hypoxia': hypoxia,  # 低酸素血症の有無
    'Severe': severe_cases  # 重症化の有無
})

# 年齢層ごとにデータを層化
df['AgeGroup'] = pd.cut(df['Age'], bins=[20, 30, 40, 50, 60, 70, 80], labels=['20-30', '30-40', '40-50', '50-60', '60-70', '70-80'])

# 各要因と重症化率の関係を可視化
factors = ['Comorbidities', 'Smoking', 'Obesity', 'Hypoxia', 'Gender']

for factor in factors:
    plt.figure(figsize=(12, 6))
    factor_severity_rate = df.groupby(['AgeGroup', factor])['Severe'].mean().reset_index()
    sns.barplot(x='AgeGroup', y='Severe', hue=factor, data=factor_severity_rate, palette='viridis')
    plt.title(f'Severity Rate by {factor.capitalize()} and Age Group')
    plt.ylabel('Severity Rate')
    plt.xlabel('Age Group')
    plt.show()

```


    
![png](output_35_0.png)
    



    
![png](output_35_1.png)
    



    
![png](output_35_2.png)
    



    
![png](output_35_3.png)
    



    
![png](output_35_4.png)
    

